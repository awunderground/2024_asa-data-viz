---
title: "Data Viz and Regression in R"
author: "Aaron R. Williams"
output:
  html_document:
    code_folding: show
    toc: TRUE
    toc_float: TRUE
    css: !expr here::here("www", "web_report.css")
editor_options:
  chunk_output_type: console
---

```{css echo=FALSE}
h1 {
    font-size: 34px;
    color: #1696d2;
}
p {
    margin: 20px 0 20px;
}
```

```{r message = FALSE, warning = FALSE, echo = FALSE}
library(tidyverse)
library(modelr)
library(broom)

theme_set(theme_minimal())

options(scipen = 999)

```

This guide is a brief introduction to code for implementing and visually evaluating regression models in R. It does not address the theory behind regression models. Data visualization is data munging. As we will quickly discover, the key to visualizing regression models is understanding how to get the right data into the right format. We will explore a few different approaches:

1. Create models with `ggplot2`
2. Use Base R to Explore `lm` Objects
3. Extract Model Data with `broom`
4. Create Data with `library(modelr)`

-----

# 1. Create Models with `ggplot2`

## `geom_smooth()`

We've already seen `geom_smooth()`, which adds a LOESS regression line with fewer than 1,000 observations and smoothing regression splines with 1,000 or more observations. 

```{r}
cars %>%
  ggplot(mapping = aes(x = speed, y = dist)) +
  geom_point() +
  geom_smooth()

```

We can change the method to any of `"lm"`, `"glm"`, `"gam"`, or `"loess"`.

```{r}
cars %>%
  ggplot(mapping = aes(x = speed, y = dist)) +
  geom_point() +
  geom_smooth(method = "lm")

```

We can also change the formula and toggle off the standard error. 

```{r}
cars %>%
  ggplot(mapping = aes(x = speed, y = dist)) +
  geom_point() +
  geom_smooth(
    method = "lm", 
    formula = y ~ log(x), 
    se = FALSE
  )

```

This final example is concise. It uses `log(x)` as a predictor but still shows the `x` axis in linear units. 

`geom_smooth()` is useful for exploratory data analysis, but it is a little limiting. Next, we will consider developing models, cleaning the data, and making data visualizations as distinct steps.

-----

# 2. Use Base R to Explore `lm` Objects

## `lm()`

`lm()` fits linear regression models in R. Here is a simple linear regression model estimated on the `cars` data with stopping distance as the dependent variable and speed as the independent variable. 

```{r}
stopping_model <- lm(formula = dist ~ speed, data = cars)

class(stopping_model)

stopping_model

```

The `lm()` function creates an object of class `"lm"`. Many R functions have convenient (generic) methods for this object that are useful for understanding and using the output of a regression model. 

-----

## `summary()`

`summary()` returns a regression table with the call, a five number summary of the residuals, coefficient estimates, standard errors, t statistics, p-values, the residual standard error, $R^2$, adjusted $R ^ 2$, the F-statistic, and the p-value for the F-statistic. 

```{r}
summary(stopping_model)

```

-----

## `coef()`

For example, `coef()` returns the coefficients. 

```{r}
coef(stopping_model)

```

-----

## `resid()`

`resid()` can be used to select just a vector of the residuals. 

```{r}
resid(stopping_model)[1:10]

```

-----

## `plot()`

`plot()` will return four plots with regression diagnostics. 

* **(1) Residual plot:** This demonstrates if the residuals have non-linear patterns or non-constant variance. 
* **(2) Normal QQ plot:** This demonstrates if the residuals are normally distributed. 
* **(3) Scale-Location plot:** This also demonstrates if the residuals have non-constant variance.
* **(4): Residuals vs. leverage plot** This demonstrates cases that may be influential.

```{r}
plot(stopping_model)

```

`plot()` with `lm()` is quick but the results are not attractive and customizing anything is a huge pain. 

-----

# 3. Extract Model Data with `broom`

To leverage the full power of `ggplot2`, we need tidy data frames with our data of interest. `library(broom)` will quickly give us access to these data!

## `library(broom)`

`library(broom)` contains three helpful functions for tidying the output of estimated models. `library(broom)` is extensible and has methods for many models (`lm()`, `glm()`, `kmeans()`, `LDA()`). We will demonstrate applications with `lm()`:

* `augment()` returns one row per observation in the estimation data and includes information like predicted values and residuals. 
* `tidy()` returns one row per coefficient and includes information like point estimates and standard errors. 
* `glance()` returns one row per model and includes information like $R^2$.

-----

### `augment()`

`augment()` returns observation-level diagnostics like residuals and hat values. 

```{r}
augment(stopping_model)

```

### Residual plot

A residual plot compares fitted values and residuals. It is a useful diagnostic to see if there are non-linear patterns that are not captured by the model and to check for constant error variance.

### `r kableExtra::text_spec("Exercise 1", color = "#1696d2")`

Let's estimate a model using a subset of the diamonds data set and then create a residual plot. 

```{r}
# sample 300 observations and set ordinal factors to nominal
set.seed(20200622)

diamonds <- diamonds %>%
  slice_sample(n = 300) %>%
  mutate(across(where(is.factor), .fns = as.character))

```

```{r}
# estimate a multiple linear regression model
diamonds_model1 <- lm(formula = price ~ carat + cut, data = diamonds)

class(diamonds_model1)

```

<font color="#55b748">**Step 1:**</font> Run the above code to estimate a linear regression model on a subset of the `diamonds` data. 

<font color="#55b748">**Step 2:**</font> Use `augment()` to create a data frame with one row per observation in the training data. 

<font color="#55b748">**Step 3:**</font> Create a scatter plot to compare `.fitted` and `.resid`. Add `geom_smooth()`. 

-----

### `tidy()`

`tidy()` returns coefficient-level diagnostics like standard errors and p-values.

```{r}
tidy(stopping_model)

```

### Coefficient plot

Here's a simple plot of estimated OLS coefficients and their 95% confidence intervals. 

```{r}
diamonds_model1_coefs <- tidy(
  diamonds_model1, 
  conf.int = TRUE,
  conf.level = 0.95
) 

diamonds_model1_coefs %>%
  ggplot(aes(x = estimate, 
             y = term,
             xmin = conf.low,
             xmax = conf.high)) +
  geom_vline(xintercept = 0) +
  geom_pointrange() +
  scale_x_continuous(
    limits = c(-10000, 10000),
    labels = scales::dollar
  )

```

-----

### `r kableExtra::text_spec("Exercise 2", color = "#1696d2")`

Let's estimate a second regression model on the `diamonds` data set and then compare the models with a coefficient plot. 

<font color="#55b748">**Step 1:**</font> Create `diamonds_model2` with `price` as the dependent variable and `carat`, `cut`, and `x` as independent variables. 

<font color="#55b748">**Step 2:**</font> Use `tidy()` to create `diamonds_model2_coefs`. Combine the results using the following code:

```{r eval = FALSE}
models_coefs <- bind_rows(
  `model1` = diamonds_model1_coefs,
  `model2` = diamonds_model2_coefs,
  .id = "model"
)

```

<font color="#55b748">**Step 3:**</font> Create a coefficient plot with `models_coefs`. Include `color = model`.

<font color="#55b748">**Step 4:**</font> Add `position = position_dodge(width = 0.5)` to `geom_pointrange()`. 

-----

Michael Correll and Michael Gleicher have an interesting paper ([preprint here](https://graphics.cs.wisc.edu/Papers/2014/CG14/Preprint.pdf)) called "Error Bars Considered Harmful: Exploring Alternate Encodings for Mean and Error". Consider this figure from their paper:

```{r echo = FALSE}
knitr::include_graphics(here::here("www", "images", "error-bars.png"))

```

Here, (a) suffers from within the bar bias, and (a) and (b) suffers from issues with binary interpretation. It's tricky to fully adopt (c) or (d), which are visually symmetric and visually continuous, but I never use bars for coefficients and I never use the "capital I" error bars. 

-----

### `glance()`

`glance()` returns model-level diagnostics like $R^2$ and $\hat{\sigma}$. 

```{r}
glance(stopping_model)

```

It isn't interesting to visualize one model using `glance()`. However, `glance()` allows for the comparison of many models. 

### Detailed Example: Creating Many models

[Hadley Wickham gave a great talk about estimating many models to the The Edinburgh R User Group.](https://www.youtube.com/watch?v=rz3_FDVt9eg) (The relevant sections begins around the 28-minute mark). Here is an example based on his talk:

```{r}
library(gapminder)

glimpse(gapminder)

```

The gapminder data set contains information about life expectancy, population, and per-capita GDP over time for every country in the world. It comes from Hans Rosling and the [Gapminder Foundation](https://www.gapminder.org/data/). 

We can estimate a simple linear regression for every country in the data set with `year` as the predictor and `lifeExp` as the outcome variable. 

```{r}
# estimate a linear model for each country
many_models <- gapminder %>%
  group_by(country, continent) %>%
  # create a nested data frame for each county
  nest(data = c(year, lifeExp, pop, gdpPercap)) %>%
  # iterate down each row and estimate a model with the nested data frame
  mutate(
    model = map(
      .x = data, 
      .f = ~glance(lm(formula = lifeExp ~ year, data = .x))
    )
  ) %>%
  ungroup()
    
# extract r^2 from each model
many_models_results <- many_models %>%
  mutate(r_squared = map_dbl(model, "r.squared"))

# plot
many_models_results %>%
  # reorder the data based on r_squared
  mutate(country = forcats::fct_reorder(.f = country, .x = r_squared)) %>%
  ggplot(mapping = aes(r_squared, country, color = continent)) +
  geom_point(alpha = 0.5)

```

`map()` functions come from `library(purrr)` and are based on the Map-Reduce framework. This is a functional approach to iteration that replaces for loops. I recommend reading more [here](https://r4ds.had.co.nz/iteration.html). 

Categorical variables are displayed in alphanumeric order by default. `fct_reorder()` from `library(forcats)` converts `country` to a factor and orders it based on the values of `r_squared`. `library(forcats)`has several useful functions for ordering categorical axes with `library(ggplot2)`.

Let's clean this up a little:

```{r fig.height = 4.5, fig.width = 5}
bind_rows(
  `High R-Squared` = slice_max(many_models_results, r_squared, n = 15),
  `Low R-Squared` = slice_min(many_models_results, r_squared, n = 15),
  .id = "R-Squared"
) %>%
  mutate(country = forcats::fct_reorder(.f = country, .x = r_squared)) %>%
  ggplot(mapping = aes(r_squared, country, color = continent)) +
  geom_point(alpha = 0.5) +
  facet_wrap(~ `R-Squared`, nrow = 2, scales = "free_y")

```

### `r kableExtra::text_spec("Exercise 3", color = "#1696d2")`

<font color="#55b748">**Step 1:**</font> Like above, estimate many models on the `diamonds` data set. Group by color. You will need to list columns in the `nest()` functions. Use `formula = price ~ carat + cut`.

<font color="#55b748">**Step 2:**</font> Extract $R^2$ from each model.

<font color="#55b748">**Step 3:**</font> Visualize the $R^2$ with a scatter plot with `r_squared` on the x axis and `color` on the y axis.

<font color="#55b748">**Step 4:**</font> Add `scale_x_continuous(limits = c(0, 1))`.

-----

# 4. Create Data with `library(modelr)`

`library(modelr)` has many useful functions for modeling. It works with more types of models than just linear models from `lm()`. 

## `add_predictions()`

`add_predictions()` adds predictions to a data set using an estimated model object. 

```{r}
add_predictions(data = diamonds, model = diamonds_model1)
```

## `add_residuals()`

`add_residuals()` adds residuals to a data set using an estimated model object. 

```{r}
add_residuals(data = diamonds, model = diamonds_model1)
```

## `data_grid()`

`data_grid()` creates an evenly-spaced grid of points using the range of observed predictors in a data set. This is useful for visualization and is really, really useful for understanding generalized linear models. `seq_range()` can be used with `data_grid()` to add a finer grid of values.  

```{r}
data_grid(data = diamonds, carat, cut) %>%
  add_predictions(diamonds_model1)

```

```{r}
cut_levels <- c("Fair", "Good",  "Very Good", "Ideal", "Premium")

data_grid(data = diamonds, carat, cut) %>%
  add_predictions(diamonds_model1) %>%
  mutate(cut = factor(cut, levels = cut_levels)) %>%
  ggplot(aes(x = carat, y = pred, color = cut)) +
  geom_line(alpha = 0.5) +
  scale_x_continuous(
    limits = c(0, 3),
    expand = c(0, 0)
  ) +
  scale_y_continuous(
    limits = c(-5000, 20000),
    expand = c(0, 0),
    labels = scales::dollar
  ) +
  labs(title = "data_grid() is useful for interpreting a regression line")

```

Categorical variables are displayed in alphanumeric order by default. Here, we use `factor()` to give cut a meaningful order: "Fair", "Good", "Very Good", "Ideal", and "Premium". 

-----

## Complex Models and GLMs

`library(modelr)` is useful for visualizing complex models such as polynomial regression or generalized linear models (GLMs) like logistic regression. 

```{r}
set.seed(20201005)

# simulate a predictor
x1 <- runif(n = 1000, min = 0, max = 10)

# simulate the outcome and create a tibble
sim_data <- bind_cols(
  x1 = x1,
  y = 10 * sin(x1) + 20 + rnorm(n = length(x1), mean = 0, sd = 2)
)

# plot
sim_data %>%
  ggplot(aes(x1, y)) +
  geom_point(alpha = 0.1)

```

Let's fit a 4th-degree polynomial and then add a line for the conditional mean. We could also use `augment()` in this case because the `x1` is dense. If there are gaps in predictors, then `data_grid()` is necessary. 

```{r}
# fit a model with a 4th-degree polynomial
lm_4 <- sim_data %>%
  lm(formula = y ~ poly(x1, degrees = 4, raw = TRUE), data = .)

# create a grid andd predictions
conditional_mean <- data_grid(sim_data, x1) %>%
  add_predictions(lm_4)

# plot
ggplot() +
  geom_point(
    data = sim_data,
    mapping = aes(x1, y),
    alpha = 0.1
  ) +
  geom_line(
    data = conditional_mean,
    mapping = aes(x1, pred),
    color = "red"
  )

```

-----

### `r kableExtra::text_spec("Exercise 4", color = "#1696d2")`

This example will demonstrate plotting predicted probabilities for a simple logistic regression model.

<font color="#55b748">**Step 1:**</font> Run the following to create a binary outcome variable. 

```
cars <- cars %>%
  mutate(crash = as.numeric(dist > 25))

```

<font color="#55b748">**Step 2:**</font> Using `lm()`, estimate a linear regression model (linear probability model) with `crash` as the outcome variable and `speed` as the predictor. Call it `cars_lm`.

<font color="#55b748">**Step 3:**</font> Run the following to estimate a logistic regression model: 

```
cars_glm <- glm(factor(crash) ~ speed, data = cars, family = "binomial")

```

<font color="#55b748">**Step 4:**</font> Use `data_grid()` to create a new data frame. Use `speed = seq_range(speed, 1000)` to make a consistent grid of values for `speed`. 

<font color="#55b748">**Step 5:**</font> Create a data frame with conditional probabilities for both models with the following code:

```
models <- data_grid(cars, speed = seq_range(speed, 1000)) %>%
  add_predictions(cars_lm, var = "lm") %>%
  add_predictions(cars_glm, type = "response", var = "glm")
```

<font color="#55b748">**Step 6:**</font> Plot the predicted probabilities for the linear probability model and logistic regression model. I used three layers: `geom_point()`, `geom_line()` with `y = lm`, and `geom_line()` with `y = glm`.

# Summary

Hopefully these notes demonstrate the power of using a tool (R) that has powerful modeling tools and powerful visualization tools. There are several approaches to visualizing regression models. 

We can model with `ggplot2` or use base R to quickly visualize our data. With a little more work, we can use `library(broom)` and `library(modelr)` to create tidy data frames and leverage the full power of `ggplot2`. 

1. Create models with `ggplot2`
2. Use Base R to Explore `lm` Objects
3. Extract Model Data with `broom`
4. Create Data with `library(modelr)`
