[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Visualization with R",
    "section": "",
    "text": "Abstract\nA reproducible research workflow should generate the same results from the same inputs every time. Unfortunately, software changes, key documentation is skipped, and that hard drive from graduate school disappeared during that last move. Reproducible research should be a minimum expectation of computational science, but too many researchers lack the tools to embrace a fully reproducible workflow. This full-day course aims to equip researchers with fundamental tools for reproducible research. The course will introduce Quarto, Git and GitHub, coding best practices, and environment management with renv through hands-on exercises and clear resources. Attendees will leave equipped to weather constantly changing software versions, documentation will be too fun to skip, and even a missing hard drive won’t ruin years of work. The course focuses on R but the content is broadly applicable."
  },
  {
    "objectID": "chapters/01_introduction-and-motivation.html#who-i-am",
    "href": "chapters/01_introduction-and-motivation.html#who-i-am",
    "title": "1  Introduction and Motivation",
    "section": "1.1 Who I Am",
    "text": "1.1 Who I Am\n\n\n\nReggee and Aaron\n\n\n\n1.1.1 Background\n\nLead Data Scientist for Statistical Computing at the Urban Institute\nAdjunct Professor in the McCourt School of Public Policy at Georgetown University\nAmerican Statistical Association Traveling Course Instructor\n\n\n\n1.1.2 R Projects\n\nSynthetic data generation (rstudio::conf(2022) talk about library(tidysynthesis))\nFormal privacy/differential privacy evaluation\n\nA Feasibility Study of Differentially Private Summary Statistics and Regression Analyses with Evaluations on Administrative and Survey Data (code) (JASA)\nBenchmarking DP Linear Regression Methods for Statistical Inference (Preprint)\n\nProjects that iterate with R Markdown/Quarto\n\nMobility Metrics data pages\nState Fiscal briefs\n\nManage the Urban Institute ggplot2 theme (Examples) (Code)\nUrban Institute R Users Group"
  },
  {
    "objectID": "chapters/01_introduction-and-motivation.html#who-are-you",
    "href": "chapters/01_introduction-and-motivation.html#who-are-you",
    "title": "1  Introduction and Motivation",
    "section": "1.2 Who Are You?",
    "text": "1.2 Who Are You?\n\nWhat types of analyses do you develop?\nWhat is your programming experience?\nWhat are you most interested to learn?"
  },
  {
    "objectID": "chapters/01_introduction-and-motivation.html#outline",
    "href": "chapters/01_introduction-and-motivation.html#outline",
    "title": "1  Introduction and Motivation",
    "section": "1.3 Outline",
    "text": "1.3 Outline\n\n1.3.1 Goals\n\nEnthusiasm\nDevelop a firm foundation with R\nLeave with enough understanding and resources that you can apply the covered material to your own work\n\nYou will still need to look stuff up!\nI will try to give you hints for where to find help\n\n\n\n\n1.3.2 Process\n\nPlease consider turning on your cameras.\nPlease ask questions at any time. You can speak up, raise your hand, or drop it in the chat.\nI need to know how you are doing. Please ask lots of questions and give your reactions.\nI will check in during breaks about pacing and content.\nWe will skip some exercises. Don’t worry, I’ve shared solutions to all exercises!"
  },
  {
    "objectID": "chapters/01_introduction-and-motivation.html#content",
    "href": "chapters/01_introduction-and-motivation.html#content",
    "title": "1  Introduction and Motivation",
    "section": "1.4 Content",
    "text": "1.4 Content\n\nIntroductions and Motivation\nGrammar of Graphics\nJon Schwabish’s Five Guidelines for Better Data Visualizations\nVisualizing big data\nVisualizing regression models\nData munging for visualization\nVisualizing time series data"
  },
  {
    "objectID": "chapters/01_introduction-and-motivation.html#why-data-visualization",
    "href": "chapters/01_introduction-and-motivation.html#why-data-visualization",
    "title": "1  Introduction and Motivation",
    "section": "1.5 Why Data Visualization?",
    "text": "1.5 Why Data Visualization?\n\nData visualization is exploratory data analysis (EDA)\nData visualization is diagnosis and validation\nData visualization is communication"
  },
  {
    "objectID": "chapters/01_introduction-and-motivation.html#why-ggplot2",
    "href": "chapters/01_introduction-and-motivation.html#why-ggplot2",
    "title": "1  Introduction and Motivation",
    "section": "1.6 Why ggplot2",
    "text": "1.6 Why ggplot2\n\n1.6.1 1. Looks good!\nlibrary(ggplot2) is used by fivethirtyeight, Financial Times, BBC, the Urban Institute, and more.\n\n\n1.6.2 2. Flexible and expressive\nBy breaking data visualization into component parts, library(ggplot2) is a set of building blocks instead of a set of rigid cookie cutters.\n\n\n1.6.3 3. Reproducible and Transparent\nI believe in a code-first approach to data analysis.\n\n\n\n\n\nCode maximizes the chance of catching mistakes when they inevitably happen and code is the clearest way to document and share an analysis.\n\n\n1.6.4 4. Scalable\nIt’s almost as easy to make the 100th chart as it is to make the 2nd chart. This allows for iteration.\n\n\n1.6.5 5. In my analysis workflow\n\n\n\n\n\nData visualization is fundamental to EDA, statistical modeling, and basically any work with data. Too many people find themselves using different tools for data visualization and statistical modeling. R/ggplot2 allows everything to happen in the same script at the same time.\nToo often, switching from a programming language to Excel, results in parsing errors or cell-reference errors."
  },
  {
    "objectID": "chapters/01_introduction-and-motivation.html#r-markdown",
    "href": "chapters/01_introduction-and-motivation.html#r-markdown",
    "title": "1  Introduction and Motivation",
    "section": "1.7 R Markdown",
    "text": "1.7 R Markdown\n\nRStudio Tutorial\nQuarto Tutorial\n\nThis short course will rely on R Markdown, which is a literate statistical programming framework that combines text and images, code, and code output into output documents like PDFs and web pages. It is like an easier-to-use LaTeX with more flexibility. Instead of .R scripts, we will use .Rmd scripts.\n\nMarkdown\nYAML Header\nCode chunks\n\n\n1.7.1 Running code in documents\nWe will mostly run code inside of .Rmd documents.\n\nRun the code like a .R script\nRun the entire current chunk \nRun all chunks above \n\n\n\n1.7.2 Knitting documents\nMore commonly, documents are knitted. This runs all of the code in the .Rmd in a new R session and then creates an output document like a .html or a .pdf. If the code has errors, knitting will fail.\nClick  when a .Rmd document is open in RStudio to knit the document.\n\n\n\n\n\n\nExercise 0\n\n\n\nStep 1: Open RStudio by double-clicking 2024_asa-data-viz.Rproj\nStep 2: Open 02_workbook.Rmd in RStudio. Make sure it is in 2024_asa-data-viz.Rproj. That is, you should not see Project: (None) in the top right of RStudio.\nStep 3: Click knit!"
  },
  {
    "objectID": "chapters/02_grammar-of-graphics.html",
    "href": "chapters/02_grammar-of-graphics.html",
    "title": "2  Grammar of Graphics",
    "section": "",
    "text": "3 Review"
  },
  {
    "objectID": "chapters/02_grammar-of-graphics.html#the-tidy-approach",
    "href": "chapters/02_grammar-of-graphics.html#the-tidy-approach",
    "title": "2  Grammar of Graphics",
    "section": "2.1 The Tidy Approach",
    "text": "2.1 The Tidy Approach\n\n2.1.1 Opinionated software\n\nOpinionated software is a software product that believes a certain way of approaching a business process is inherently better and provides software crafted around that approach. ~ Stuart Eccles\n\n\n\n2.1.2 Tidy data\nThe defining opinion of the tidyverse is its wholehearted adoption of tidy data. Tidy data has three features:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a dataframe. (This is from the paper, not the book)\n\n\n\n\n\n\nSource: R for Data Science\nTidy data was formalized by Hadley Wickham in “Tidy Data” in the Journal of Statistical Software in 2014. It is equivalent to Codd’s 3rd normal form (Codd, 1990) for relational databases.\n\nTidy datasets are all alike, but every messy dataset is messy in its own way. ~ Hadley Wickham\n\nThe tidy approach to data science is powerful because it breaks data work into two distinct parts.\n\nFirst, get the data into a tidy format.\nSecond, use tools optimized for tidy data.\n\nBy standardizing the data structure for most community-created tools, the framework oriented diffuse development and reduced the friction of data work."
  },
  {
    "objectID": "chapters/02_grammar-of-graphics.html#grammar-of-graphics",
    "href": "chapters/02_grammar-of-graphics.html#grammar-of-graphics",
    "title": "2  Grammar of Graphics",
    "section": "2.2 Grammar of Graphics",
    "text": "2.2 Grammar of Graphics\nggplot2 is an R package for data visualization that was developed during Hadley Wickham’s graduate studies at Iowa State University. ggplot2 is formalized in “A Layered Grammar of Graphics” by Hadley Wickham, which was published in the Journal of Statistical Software in 2010.\nThe grammar of graphics, originally by Leland Wilkinson, is a theoretical framework that breaks all data visualizations into their component pieces. With the layered grammar of graphics, Wickham extends Wilkinson’s grammar of graphics and implements it in R. The cohesion is impressive and the theory flows to the code which informs the data visualization process in a way not reflected in any other data viz tool.\nThere are eight main ingredients to the grammar of graphics. We will work our way through the ingredients with many hands-on examples.\n\n\n\n\n\n\n\nExercise 0\n\n\n\nStep 1: Open 2024_asa-data-viz.Rproj.\nStep 2: Open 02_workbook.Rmd.\n\n\n\n\n\n\n\n\n\nExercise 1\nStep 1: Type (don’t copy & paste) the following code below library(tidyverse) in the new chunk where it says # YOUR WORK GOES HERE.\n\nggplot(data = storms) + \n  geom_point(mapping = aes(x = pressure, y = wind))\n\nStep 2: Add a comment above the ggplot2 code that describes the plot we created.\nStep 3: As we progress, add comments below the data visualization code that describe the argument or function that corresponds to each of the first three components of the grammar of graphics.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n1 Data are the values represented in the visualization.\nggplot(data = ) or data %&gt;% ggplot()\n\nstorms %&gt;%\n  select(name, year, category, lat, long, wind, pressure)\n\n# A tibble: 19,537 × 7\n   name   year category   lat  long  wind pressure\n   &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n 1 Amy    1975       NA  27.5 -79      25     1013\n 2 Amy    1975       NA  28.5 -79      25     1013\n 3 Amy    1975       NA  29.5 -79      25     1013\n 4 Amy    1975       NA  30.5 -79      25     1013\n 5 Amy    1975       NA  31.5 -78.8    25     1012\n 6 Amy    1975       NA  32.4 -78.7    25     1012\n 7 Amy    1975       NA  33.3 -78      25     1011\n 8 Amy    1975       NA  34   -77      30     1006\n 9 Amy    1975       NA  34.4 -75.8    35     1004\n10 Amy    1975       NA  34   -74.8    40     1002\n# ℹ 19,527 more rows\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n2 Aesthetic mappings are directions for how variables from the data are mapped to visual elements in the data visualization. Aesthetic mappings show variation in the data through variation in the data visualization. Aesthetic mappings include linking variables to the x-position, y-position, color, fill, shape, transparency, and size.\naes(x = , y = , color = )\n\n\nX or Y\n\n\n\n\n\nColor or Fill\n\n\n\n\n\n\n\n\n\n\nSize\n\n\n\n\n\nShape\n\n\n\n\n\nOthers: transparency, line type\n\n\n\n\n\n\nTip\n\n\n\n3 Geometric objects are representations of the data, including points, lines, and polygons.\nPlots are often called their geometric object(s).\n\n\ngeom_bar() or geom_col()\n\n\n\n\n\ngeom_line()\n\n\n\n\n\ngeom_point()\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nStep 1: Duplicate the code from exercise 1. Inside aes(), add color = category. Run the code.\nStep 2: Replace color = category with color = \"green\". Run the code. What changed? Is this unexpected?\nStep 3: Remove color = \"green\" from aes() and add it inside inside of geom_point() but outside of aes(). Run the code.\nStep 4: This is a little cluttered. Add alpha = 0.2 inside geom_point() but outside of aes().\n\n\nAesthetic mappings like x and y almost always vary with the data. Aesthetic mappings like color, fill, shape, transparency, and size can vary with the data. But those arguments can also be added as styles that don’t vary with the data. If you include those arguments in aes(), they will show up in the legend (which can be annoying! and is also a sign that something should be changed!).\n\n\n\n\n\n\n\nExercise 3\n\n\n\nStep 1: Create a new scatter plot using the msleep data set. Use bodywt on the x-axis and sleep_total on the y-axis.\nStep 2: The y-axis doesn’t contain zero. Below geom_point(), add scale_y_continuous(limits = c(0, NA)). Hint: add + after geom_point().\nStep 3: The x-axis is clustered near zero. Add scale_x_log10() above scale_y_continuous(limits = c(0, NA)).\nStep 4: Add and run options(scipen = 999). Rerun the code from steps 1-3.\n\n\n\n\n\n\n\n\nTip\n\n\n\n4 Scales control the exact behaviors of aesthetic mapping. scale_*_*() functions can change:\n\nThe range and labels on the x-axis and y-axis\nThe colors used for color and fill\nThe sizes of shapes\nShapes\n\nThere are dozens of scale functions and their names follow a formula:\n\nThey all start with scale_.\nNext, comes the name of the aesthetic for the scale (i.e. x, y, fill, size, etc.).\nFinally, comes the type of variable or transformation (i.e. discrete, continuous, and reverse).\n\nscale_x_continuous() and scale_y_continuous() are two popular scale_*_*() functions.\n\n\n\nBefore\nscale_x_continuous()\n\n\n\n\n\n\n\nAfter\nscale_x_reverse()\n\n\n\n\n\n\n\nBefore\nscale_size_continuous(breaks = c(25, 75, 125))\n\n\n\n\n\n\n\nAfter\nscale_size_continuous(range = c(0.5, 20), breaks = c(25, 75, 125))\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\nStep 1: Type the following code in your script.\ndata &lt;- tibble(x = 1:10, y = 1:10)\nggplot(data = data) +\n  geom_blank(mapping = aes(x = x, y = y))\nStep 2: Add coord_polar() to your plot.\nStep 3: Add labs(title = \"Polar coordinate system\") to your plot.\n\n\n\n\n\n\n\n\nTip\n\n\n\n5 Coordinate systems map scaled geometric objects to the position of objects on the plane of a plot. The two most popular coordinate systems are the Cartesian coordinate system and the polar coordinate system.\n\n\n\n\n\n\n\ncoord_polar()\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\nStep 1: Create a scatter plot of the storms data set with pressure on the x-axis and wind on the y-axis.\nStep 2: Add facet_wrap(~ month)\n\n\n\n\n\n\n\n\nTip\n\n\n\n6 Facets (optional) break data into meaningful subsets. facet_wrap(), facet_grid(), and facet_geo().\n\n\n\n\nfacet_wrap()\nfacet_wrap(~ category)\n\n\n\n\n\n\n\nfacet_grid()\nfacet_grid(month ~ year)\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 6\n\n\n\nStep 1: Add the following code to your script. Submit it!\nggplot(storms) +\n  geom_bar(mapping = aes(x = category))\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n7 Statistical transformations (optional) transform the data, typically through summary statistics and functions, before aesthetic mapping.\nBefore transformations, each observation in data is represented by one geometric object (i.e. a scatter plot). After a transformation, a geometric object can represent more than one observation (i.e. a bar in a histogram).\n\n\nNote: geom_bar() performs statistical transformation. Use geom_col() to create a column chart with bars that encode individual observations in the data set.\n\n\n\n\n\n\n\nExercise 7\n\n\n\nStep 1: Duplicate Exercise 6.\nStep 2: Add theme_minimal() to the plot.\n\n\n\n\n2.2.1 Themes\n\n\n\n\n\n\nTip\n\n\n\n8 Theme controls the visual style of plot with font types, font sizes, background colors, margins, and positioning.\n\n\n\nDefault theme\n\n\n\n\n\n\n\nTheme Minimal\n\n\n\n\n\n\n\nfivethirtyeight theme\n\n\n\n\n\n\n\nurbnthemes\n\n\n\n\n\nIf you prefer the minimal theme, you can add theme_minimal() to each visualization or add theme_set(theme_minimal) at the beginning of your script.\n\n\n\n\n\n\n\nExercise 8\n\n\n\nStep 1: Add the following exercise to you script. Run it!\nstorms %&gt;%  \n  filter(category &gt; 0) %&gt;%\n  distinct(name, year) %&gt;%\n  count(year) %&gt;%\n  ggplot() + \n  geom_line(mapping = aes(x = year, y = n))\nStep 2: Add geom_point(mapping = aes(x = year, y = n)) after geom_line().\n\n\n\n\n\n\n\n\nTip\n\n\n\nLayers allow for distinct geometric objects and/or distinct data sets to be combined in the same data visualization.\n\n\n\n\n\n\n\n\n\nExercise 9\n\n\n\nStep 1: Add the following exercise to you script. Run it!\nggplot(data = storms, mapping = aes(x = pressure, y = wind)) + \n  geom_point() +\n  geom_smooth()\n\n\n\n\n\n\n\n\nTip\n\n\n\nInheritances pass aesthetic mappings from ggplot() to later geom_*() functions.\n\n\nNotice how the aesthetic mappings are passed to ggplot() in example 9. This is useful when using layers!\n\n\n\n\n\n\n\nExercise 10\n\n\n\nStep 1: Pick your favorite plot from exercises 1 through 9 and duplicate it in a new code chunk.\nStep 2: Add ggsave(filename = \"favorite-plot.png\") and then look at the saved file.\nStep 3: Add width = 6 and height = 4 to ggsave()."
  },
  {
    "objectID": "chapters/02_grammar-of-graphics.html#resources",
    "href": "chapters/02_grammar-of-graphics.html#resources",
    "title": "2  Grammar of Graphics",
    "section": "3.1 Resources",
    "text": "3.1 Resources\n\nUrban Institute R Users Group website\nWhy the Urban Institute visualizes data with ggplot2\nR for Data Science: data visualization\nawunderground themes\nR Graph Gallery"
  },
  {
    "objectID": "chapters/03_five-guidelines.html#better-data-visualizations",
    "href": "chapters/03_five-guidelines.html#better-data-visualizations",
    "title": "3  Jon Schwabish’s Five Guidelines in R",
    "section": "3.1 Better Data Visualizations",
    "text": "3.1 Better Data Visualizations\nJon Schwabish wrote a book called Better Data Visualizations. Chapter two includes five guidelines for better data visualizations. We will work through the five guidelines with examples in library(ggplot2). Jon has taught me a ton about data viz and I taught Jon how to use R."
  },
  {
    "objectID": "chapters/03_five-guidelines.html#guideline-1-show-the-data",
    "href": "chapters/03_five-guidelines.html#guideline-1-show-the-data",
    "title": "3  Jon Schwabish’s Five Guidelines in R",
    "section": "3.2 Guideline 1: Show the Data",
    "text": "3.2 Guideline 1: Show the Data\n\n“Your reader can only grasp your point, argument, or story if they see the data.”\n\nSchwabish focuses on communications, but this rule holds true for analysis too. Let’s consider the classic example of Anscombe’s quartet.\n\n\n\n\n\n\n\nExercise 1\n\n\n\nStep 1: Copy-and-paste the following code to get the Anscombe’s quartet data:\n\nlibrary(tidyverse)\n\ntheme_set(theme_minimal())\n\ntidy_anscombe &lt;- \n  anscombe %&gt;%\n  # make the wide data too long\n  pivot_longer(\n    cols = everything(), \n    names_to = \"names\", \n    values_to = \"value\"\n  ) %&gt;%\n  # split the axis and quartet id\n  mutate(\n    coord = str_sub(names, start = 1, end = 1),\n    quartet = str_sub(names, start = 2, end = 2) \n  ) %&gt;%\n  group_by(quartet, coord) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  # make the data tidy\n  pivot_wider(id_cols = c(id, quartet), names_from = coord, values_from = value) %&gt;%\n  ungroup() %&gt;%\n  select(-id)\n\nStep 2: Create a data visualization with x = x, y = y, and geom_smooth(method = \"lm\", se = FALSE). The plot should have one upward sloping line.\nStep 3: Facet wrap the plot based on quartet. The plot should have four panels with lines with identical slopes and intercepts.\nStep 4: Add geom_point().\n\n\nThe four data sets have identical mean and sample variance for x, and nearly identical mean of y, sample variance of y, correlation between x and y, regression line, and coefficient of determination.\nThere is value in exploring and showing the data instead of relying exclusively on summaries of the data! This generalizes to a bunch of use cases and demonstrates the value of layers. For instance, a box and whisker plot is useful for highlighting important values in a univariate distribution and can be layered on top of a univariate dot plot.\n\nConsider an even more dramatic example by Justin Matejka and George Fitzmaurice based on the Datasaurus by Alberto Cairo. (source) Again, these data sets have identical mean and sample variance for x, and nearly identical mean of y, sample variance of y, correlation between x and y, regression line, and coefficient of determination.\n\nread_tsv(here::here(\"data\", \"DatasaurusDozen.tsv\")) %&gt;%\n  ggplot(aes(x, y)) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Similar Summaries Do Not Mean Similar Data!\") +\n  facet_wrap(~dataset)"
  },
  {
    "objectID": "chapters/03_five-guidelines.html#guideline-2-reduce-the-clutter",
    "href": "chapters/03_five-guidelines.html#guideline-2-reduce-the-clutter",
    "title": "3  Jon Schwabish’s Five Guidelines in R",
    "section": "3.3 Guideline 2: Reduce the Clutter",
    "text": "3.3 Guideline 2: Reduce the Clutter\n\n“The use of unnecessary visual elements distracts your reader from the central idea and clutters the page.”\n\nA few things to avoid:\n\nheavy tick marks and grid lines\nunnecessary 3D\nexcessive text\n\nConsider this image from Claus Wilke’s Fundamental’s of Data Visualization.\n\n\n\n\n\nHow many passengers are in first class? How many male passengers are in 3rd class? Let’s recreate this plot without the gratuitous 3D.\n\n\n\n\n\n\n\nExercise 2\n\n\n\nStep 1: Copy-and-paste the following data into your exercise document.\n\ntitanic &lt;- tribble(\n  ~Class, ~Sex, ~n,\n  \"1st class\", \"female passengers\", 144,\n  \"1st class\", \"male passengers\", 179,\n  \"2nd class\", \"female passengers\", 106,\n  \"2nd class\", \"male passengers\", 171, \n  \"3rd class\", \"female passengers\", 216,\n  \"3rd class\", \"male passengers\", 493\n)\n\nStep 2: Recreate the 3D plot in 2D."
  },
  {
    "objectID": "chapters/03_five-guidelines.html#guideline-3-integrate-the-graphics-and-text",
    "href": "chapters/03_five-guidelines.html#guideline-3-integrate-the-graphics-and-text",
    "title": "3  Jon Schwabish’s Five Guidelines in R",
    "section": "3.4 Guideline 3: Integrate the Graphics and Text",
    "text": "3.4 Guideline 3: Integrate the Graphics and Text\n\nRemove legends when possible and label data directly\nWrite active titles like newspaper headlines\nAdd explainers\n\nlabs() adds title, subtitle, caption, and tag to ggplot2 objects. It can also be used to overwrite x, y, and legend titles. Use NULL to remove a label entirely (not \"\"). ggtitle(), xlab(), and ylab() are alternatives, but I prefer to exclusively use labs() for clarity.\n\n\n\n\n\n\n\nExercise 3\n\n\n\nStep 1: Duplicate the titanic example from above.\nStep 2: Add a newspaper like headline with labs(title = \"\").\nStep 3: Add the sources of the data with caption = \"Data from library(titanic)\" in labs().\n\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\nlibrary(ggtext) is a useful library for extending text functionality in ggplot2.\nStep 1: Install ggtext with install.packages(\"ggtext\") and load the package with library(ggtext).\nStep 2: Duplicate the titanic example from above.\nStep 3: We want to compare the sexes within classes. Add position = \"dodge\" inside geom_col().\nStep 4: Add the following code:\n\n  theme(plot.title = element_markdown())\n\nStep 5: Add the following code:\n\n  labs(\n    title = \"More \n    &lt;span style='color:#00BFC4;'&gt;male passengers&lt;/span&gt; died than \n    &lt;span style='color:#F8766D;'&gt;female passengers&lt;/span&gt; in all three classes\",\n    x = NULL,\n    y = NULL\n  )\n\nStep 6: Add guides(fill = \"none\") to remove the legend.\n\n\nTip: I found this solution by Googling “add color in ggplot2 title”.\nannotate(), geom_text(), and geom_text_repel() from library(ggrepel) are useful for labeling data directly and adding explainers. Consider directly labeling bars instead of using y-axes, labeling lines instead of using legends for colors, and directly labeling points. Also, consider how an explainer or annotation layer can enhance a data visualization. This example by Neil Richards about the name Neil is a great demonstration of explainers.\nFor a publication, we could continue refining this exercise. Here is an example:\n\nlibrary(ggtext)\n\ntribble(\n  ~Class, ~Sex, ~n,\n  \"1st class\", \"female passengers\", 144,\n  \"1st class\", \"male passengers\", 179,\n  \"2nd class\", \"female passengers\", 106,\n  \"2nd class\", \"male passengers\", 171, \n  \"3rd class\", \"female passengers\", 216,\n  \"3rd class\", \"male passengers\", 493\n) %&gt;%\n  ggplot(aes(Class, n, fill = Sex)) +\n  geom_col(position = \"dodge\") +\n  geom_text(\n    aes(label = n),\n    position = position_dodge(width = 0.9),\n    vjust = -1\n  ) +\n  scale_y_continuous(limits = c(0, 550)) +\n  labs(\n    title = \"More \n    &lt;span style='color:#00BFC4;'&gt;male passengers&lt;/span&gt; died than \n    &lt;span style='color:#F8766D;'&gt;female passengers&lt;/span&gt; in all three classes\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme(\n    panel.grid = element_line(color = \"white\"),\n    plot.title = element_markdown(),\n    axis.text.y = element_blank()\n  ) +\n  guides(fill = \"none\")"
  },
  {
    "objectID": "chapters/03_five-guidelines.html#guideline-4-avoid-the-spaghetti-chart",
    "href": "chapters/03_five-guidelines.html#guideline-4-avoid-the-spaghetti-chart",
    "title": "3  Jon Schwabish’s Five Guidelines in R",
    "section": "3.5 Guideline 4: Avoid the Spaghetti Chart",
    "text": "3.5 Guideline 4: Avoid the Spaghetti Chart\n\n“Sometimes we face the challenge of including lots of data in a single graph but we don’t need to try to pack everything into a single graph.\n\nFaceting or using small multiples is a useful way to declutter a busy data visualization. We’ve already encountered faceting multiple times because it is so natural in ggplot2. With effective small multiples, if a reader understand how to read one small multiple then they should understand how to read all of the multiples. Two tips:\n\nArrange the small multiples in a logical order\nUse the same layout, size, font, and color in each small multiple\n\n\nConsider an example where we are writing about the relationship between per capita GDP and life expectancy over time in the United States and two comparison countries. By default, the facets will show up in alpha-numeric order (Canada, Mexico, United States). What if we want to change the order so the United States is first and the two comparison countries come later?\nTo do this, we just need to turn country into a factor variable with mutate() and factor().\n\nlibrary(gapminder)\n\ncountries &lt;- c(\"United States\", \"Mexico\", \"Canada\")\n\ngapminder %&gt;%\n  filter(country %in% countries) %&gt;%\n  mutate(country = factor(country, levels = countries)) %&gt;%\n  ggplot(aes(gdpPercap, lifeExp, color = country)) +\n  geom_path(color = \"grey\") +\n  geom_point() +\n  scale_x_continuous(\n    limits = c(0, 50000),\n    breaks = c(0, 20000, 40000), \n    labels = scales::dollar\n  ) + \n  facet_wrap(~ country) +\n  labs(\n    title = \"The United States has Made Less Progress in Life Expectancy,\\nEven as it has Gotten Richer\", \n    x = \"Per capita GDP\",\n    y = \"Life Expectancy\",\n    caption = \"gapminder data from 1952-2007\"\n  ) +\n  guides(color = \"none\")\n\n\n\n\nFuther, we can label the first and last years but only in the first facet.\n\nlibrary(gapminder)\nlibrary(ggrepel)\n\nyears &lt;- c(1952, 2007)\ncountries &lt;- c(\"United States\", \"Mexico\", \"Canada\")\n\ngapminder %&gt;%\n  filter(country %in% countries) %&gt;%\n  mutate(country = factor(country, levels = countries)) %&gt;%\n  mutate(\n    year = if_else(\n      condition = country == \"United States\" & year %in% years, \n      true = year,\n      false = NA_integer_\n    )\n  ) %&gt;%\n  ggplot(aes(gdpPercap, lifeExp, color = country)) +\n  geom_path(color = \"grey\") +\n  geom_point() +\n  geom_text(aes(label = year, y = lifeExp - 2), color = \"black\") +\n  scale_x_continuous(\n    limits = c(0, 50000),\n    breaks = c(0, 20000, 40000), \n    labels = scales::dollar\n  ) + \n  facet_wrap(~ country) +\n  labs(\n    title = \"The United States has Made Less Progress in Life Expectancy,\\nEven as it has Gotten Richer\", \n    x = \"Per capita GDP\",\n    y = \"Life Expectancy\",\n    caption = \"gapminder data from 1952-2007\"\n  ) +\n  guides(color = \"none\")"
  },
  {
    "objectID": "chapters/03_five-guidelines.html#guideline-5-start-with-gray",
    "href": "chapters/03_five-guidelines.html#guideline-5-start-with-gray",
    "title": "3  Jon Schwabish’s Five Guidelines in R",
    "section": "3.6 Guideline 5: Start with Gray",
    "text": "3.6 Guideline 5: Start with Gray\n\n“Whenever you make a graph, start with all gray data elements. By doing so, you force yourself to be purposeful and strategic in your use of color, labels, and other elements.”\n\nlibrary(gghighlight) complements this idea of starting with gray. Let’s consider an example using the Gapminder data.\n\n\n\n\n\n\n\nExercise 5\n\n\n\nStep 1: Install and load the gghighlight package and gapminder package.\nStep 2: Copy-and-paste the following code to create a data frame with the cumulative change in per-capita GDP in European countries:\n\ndata &lt;- gapminder %&gt;%\n  filter(continent %in% c(\"Europe\")) %&gt;%\n  group_by(country) %&gt;%\n  mutate(pcgdp_change = ifelse(year == 1952, 0, gdpPercap - lag(gdpPercap))) %&gt;%\n  mutate(pcgdp_change = cumsum(pcgdp_change))\n\nStep 3: Create a line plot with x = year, y = pcgdp_change, group = country, and geom_line().\nStep 4: Add the following code to clean up the x-axis and y-axis.\n\n  scale_x_continuous(\n    expand = expansion(mult = c(0.002, 0)),\n    breaks = c(seq(1950, 2010, 10)),\n    limits = c(1950, 2010)\n  ) +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.002)),\n    breaks = 0:8 * 5000,\n    labels = scales::dollar,\n    limits = c(0, 40000)\n  ) +\n  labs(\n    x = \"Year\",\n    y = \"Change in per-capita GDP (US dollars)\"\n  )\n\nStep 5: Suppose we want to highlight the two best-performing counties. We could add a new variable and tinker with the colors or we can use library(gghighlight). Switch group to color in you existing code and add gghighlight(max(pcgdp_change) &gt; 35000)."
  },
  {
    "objectID": "chapters/04_big-data.html#big-data",
    "href": "chapters/04_big-data.html#big-data",
    "title": "4  Visualizing Big Data",
    "section": "4.1 Big Data",
    "text": "4.1 Big Data\nOur examples thus far have focused on data sets with a modest number of observations and variables. Larger data sets can create new challenges."
  },
  {
    "objectID": "chapters/04_big-data.html#challenge-1-overplotting",
    "href": "chapters/04_big-data.html#challenge-1-overplotting",
    "title": "4  Visualizing Big Data",
    "section": "4.2 Challenge 1: Overplotting",
    "text": "4.2 Challenge 1: Overplotting\nOverplotting is when some geometric objects in a data visualization obscure other geometric objects. Overplotting is common when there is a highly frequent observation, if there is a lack of precision, or too many observations."
  },
  {
    "objectID": "chapters/04_big-data.html#challenge-2-too-many-pairwise-comparisons",
    "href": "chapters/04_big-data.html#challenge-2-too-many-pairwise-comparisons",
    "title": "4  Visualizing Big Data",
    "section": "4.3 Challenge 2: Too many pairwise comparisons",
    "text": "4.3 Challenge 2: Too many pairwise comparisons\nIf \\(m\\) is the number of variables in a data set, then there are \\(\\frac{m(m - 1)}{2}\\) pairwise relationships in a data set.\n\ntibble(m = 2:200) %&gt;%\n  mutate(`Pairwise Relationships` = m * (m - 1) / 2) %&gt;%\n  ggplot(aes(m, `Pairwise Relationships`)) +\n  geom_line() + \n  labs(\n    title = \"The Number of Pairwise Relationships Explodes in Modestly Wide Data\",\n    x = \"m (Number of predictors)\"\n  ) +\n  scale_y_continuous(labels = scales::comma)"
  },
  {
    "objectID": "chapters/04_big-data.html#overplotting",
    "href": "chapters/04_big-data.html#overplotting",
    "title": "4  Visualizing Big Data",
    "section": "4.4 Overplotting",
    "text": "4.4 Overplotting\n\n\n\n\n\n\nExercise 1\n\n\n\nA data set doesn’t need thousands of observations to have overplotting. Consider a simple example using the mpg data set from library(ggplot2).\nStep 1: Create this plot using the mpg data set with variables cyl and hwy.\n\n\n\n\n\nStep 2: Use nrow(mpg) to count the number of observations in mpg. Is there overplotting?\nStep 3: Replace geom_point() with geom_jitter(). What happens?\nStep 4: Experiment with the width and height arguments. You can see the documentation with ?geom_jitter. What seems to be the best “best” combination?\n\n\nThe first pillar in The Seven Pillars of Statistical Wisdom by Stephen Stigler identifies an interesting paradox:\n\n“By aggregating, you lose the identity of the individual, so you’re throwing away information, but you’re also gaining information of a different sort. No one wants to be reduced to a statistic, but by losing the identity of the individual, you are producing information about the group.”\n\ngeom_jitter() creates a similar paradox. Just like how we gain information by throwing out information with aggregation, we can gain clarity by introducing errors to our data with geom_jitter().\n\n\n\n\n\n\n\nExercise 2\n\n\n\nNow we’ll focus on the diamonds data set from library(ggplot2). It contains information about 53,940 diamonds.\n\nglimpse(diamonds)\n\nRows: 53,940\nColumns: 10\n$ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.…\n$ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver…\n$ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,…\n$ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, …\n$ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64…\n$ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58…\n$ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34…\n$ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.…\n$ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.…\n$ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.…\n\n\nJittering helps with overplotting with modestly sized data sets. It is not helpful with larger data sets. Let’s look at the diamonds data set with jitter:\n\nwithout_jitter &lt;- diamonds %&gt;%\n  ggplot(aes(x = carat, y = price)) +\n  geom_point() +\n  labs(subtitle = \"Without Jitter\")\n\nwith_jitter &lt;- diamonds %&gt;%\n  ggplot(aes(x = carat, y = price)) +\n  geom_jitter() +\n  labs(subtitle = \"With Jitter\")\n\nwithout_jitter + with_jitter\n\n\n\n\nStep 1: Create a scatter plot with the diamonds data set that shows the relationship between carat and price.\nStep 2: Try the following changes:\n\nChange the size of points with the size argument in geom_point().\nChange to hollow points with shape = 1 in geom_point().\nAdd transparency to points with alpha = 0.1 in geom_point().\nUse facet_wrap() and facet_grid()\nTry sampling with the following:\n\ndiamonds %&gt;% \n  slice_sample(n = 1000) %&gt;%\n  ggplot() + ...\n\nStep 3: Which do you prefer? What did you learn about the diamonds data set with these different techniques?\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\nWe’ll continue with the diamonds data set. This time we’ll experiment with some summaries instead of visualizing all observations directly.\nStep 1: Create a scatter plot with the diamonds data set that shows the relationship between carat and price.\nStep 2: Try the following changes:\n\nUse geom_hex() instead of geom_point() for multi-dimensional binning with hexagons. Experiment with different values for the argument bins.\nAdd geom_smooth() to add a model on top of the points.\n\n\n\n\n\n4.4.1 Long Data Summary\nOverplotting is a major challenge even with modestly sized data. There are at least three causes for the problem:\n\nFrequent values\nLack of precision\nMany observations\n\nWe’ve explored some solutions to overplotting, but the right solution depends on the cause of the overplotting:\n\nAdding noise works for frequent values and lack of precision, but does not work for many observations.\nFaceting can help with all three causes depending on the data.\nAdding transparency almost always helps.\nBinning the data or adding summaries doesn’t add much clarity for frequent values or lack of precision, but is essential for very large data sets.\nSampling is also a useful tool when interested in general trends, but sampling can obscure anomalies, rare events, and uncommon relationships."
  },
  {
    "objectID": "chapters/04_big-data.html#wide-data",
    "href": "chapters/04_big-data.html#wide-data",
    "title": "4  Visualizing Big Data",
    "section": "4.5 Wide Data",
    "text": "4.5 Wide Data\nTechniques for visualizing wide data, and dimension reduction more broadly, are far less settled in the literature.\n\n\n\n\n\n\n\nExercise 4\n\n\n\nApproach 1: parallel coordinate plots (Inselberg 1985)\nStep 1: Install and load the GGally package.\nStep 2: Install and load the palmerpenguins package.\nStep 3: Pipe (%&gt;%) the data into ggparcoord(columns = 2:5).\nStep 4: Add alphaLines = 0.3 inside of ggparcoord().\nStep 5: Add groupColumn = 1 inside of ggparcoord().\n\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\nApproach 2: scatterplot matrices (Carr 1985)\nStep 1: Install and load the GGally package.\nStep 2: Use select(cty, hwy, year, fl, displ) to pick a subset of variables from the mpg data set. Warning: This function will crash R if too many variables are included.\nStep 3: Run ggpairs() on the subset of variables from mpg.\n\n\n\n\n\n\n\n\n\nExercise 6\n\n\n\nHere we have a data set with 493 votes from two years of the 114th Senate (2015-2017). The data set has 100 rows and 495 columns. An affirmative vote is 1, a negative vote is -1, and an abstention is 0. The data are from Bradley Robinson and this example is based on an earlier analysis by Professor Sivan Leviyang.\nStep 1: Load the votes data with\nvotes &lt;- read_csv(here::here(\"data\", \"votes.csv\"))\n\nStep 2: Run PCA with the following code\n\n# select the numeric variables\nvotes_numeric &lt;- votes %&gt;%\n  select_if(is.numeric)\n\n# run PCA\nvotes_pca &lt;- prcomp(votes_numeric)\n\n# extract the principle components\nvotes_pcs &lt;- votes_pca %&gt;%\n  .$x %&gt;%\n  as_tibble()\n\n# combine the pcs to the names and parties\nvotes_pcs &lt;- bind_cols(\n  select(votes, name, party),\n  votes_pcs\n)\n\nsummary(votes_pca)\n\nStep 3: Use x = PC1, y = PC2, and geom_point() to plot the data.\nStep 4: Add party as color. Try labeling a few individual observations with geom_text().\nStep 5: Add x and y labels that include the proportion of variation explained by each PC.\n\n\nPCA performs linear dimension reduction. Observations are projected on to a line, plane, or hyperplane. There are non-linear dimension reduction techniques like UMAP, which projects observations on to manifolds, but there techniques are much more difficult to use and are difficult to communicate.\nOther techniques for wide data include but are not limited to:\n\nt-Distributed Stochastic Neighbor Embedding (t-SNE) (Maaten and Hinton, 2008)\nUniform Manifold Approximation and Projection (UMAP) (Mciness et al., 2018)\nGrand tours (Asimov, 1985)\nRotating plots (Cook and Miller, 2006)\n\n\n\n4.5.1 Wide Data Summary\nWide data are an even more challenging issue than overplotting. We’ve seen two options for visualizing many dimensions directly and we’ve explored one tool for dimension reduction.\n\nParallel coordinate plots\nPairwise comparison plots\nDimension reduction\n\nPCA\nt-SNE and UMAP\n\n\nSuggestion\n\nIf you have fewer than 50 variables, then look at relationships between variables and build up to a larger model of relationships.\nIf you have 50 or more variables, then start with dimension reduction and then unpack the important relationships from the dimension reduction."
  },
  {
    "objectID": "chapters/05_linear-regression.html#create-models-with-ggplot2",
    "href": "chapters/05_linear-regression.html#create-models-with-ggplot2",
    "title": "5  Data Viz and Regression in R",
    "section": "5.1 1. Create Models with ggplot2",
    "text": "5.1 1. Create Models with ggplot2\n\n5.1.1 geom_smooth()\nWe’ve already seen geom_smooth(), which adds a LOESS regression line with fewer than 1,000 observations and smoothing regression splines with 1,000 or more observations.\n\ncars %&gt;%\n  ggplot(mapping = aes(x = speed, y = dist)) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\nWe can change the method to any of \"lm\", \"glm\", \"gam\", or \"loess\".\n\ncars %&gt;%\n  ggplot(mapping = aes(x = speed, y = dist)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\nWe can also change the formula and toggle off the standard error.\n\ncars %&gt;%\n  ggplot(mapping = aes(x = speed, y = dist)) +\n  geom_point() +\n  geom_smooth(\n    method = \"lm\", \n    formula = y ~ log(x), \n    se = FALSE\n  )\n\n\n\n\nThis final example is concise. It uses log(x) as a predictor but still shows the x axis in linear units.\ngeom_smooth() is useful for exploratory data analysis, but it is a little limiting. Next, we will consider developing models, cleaning the data, and making data visualizations as distinct steps."
  },
  {
    "objectID": "chapters/05_linear-regression.html#use-base-r-to-explore-lm-objects",
    "href": "chapters/05_linear-regression.html#use-base-r-to-explore-lm-objects",
    "title": "5  Data Viz and Regression in R",
    "section": "5.2 2. Use Base R to Explore lm Objects",
    "text": "5.2 2. Use Base R to Explore lm Objects\n\n5.2.1 lm()\nlm() fits linear regression models in R. Here is a simple linear regression model estimated on the cars data with stopping distance as the dependent variable and speed as the independent variable.\n\nstopping_model &lt;- lm(formula = dist ~ speed, data = cars)\n\nclass(stopping_model)\n\n[1] \"lm\"\n\nstopping_model\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nCoefficients:\n(Intercept)        speed  \n    -17.579        3.932  \n\n\nThe lm() function creates an object of class \"lm\". Many R functions have convenient (generic) methods for this object that are useful for understanding and using the output of a regression model.\n\n\n\n5.2.2 summary()\nsummary() returns a regression table with the call, a five number summary of the residuals, coefficient estimates, standard errors, t statistics, p-values, the residual standard error, \\(R^2\\), adjusted \\(R ^ 2\\), the F-statistic, and the p-value for the F-statistic.\n\nsummary(stopping_model)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value         Pr(&gt;|t|)    \n(Intercept) -17.5791     6.7584  -2.601           0.0123 *  \nspeed         3.9324     0.4155   9.464 0.00000000000149 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 0.00000000000149\n\n\n\n\n\n5.2.3 coef()\nFor example, coef() returns the coefficients.\n\ncoef(stopping_model)\n\n(Intercept)       speed \n -17.579095    3.932409 \n\n\n\n\n\n5.2.4 resid()\nresid() can be used to select just a vector of the residuals.\n\nresid(stopping_model)[1:10]\n\n        1         2         3         4         5         6         7         8 \n 3.849460 11.849460 -5.947766 12.052234  2.119825 -7.812584 -3.744993  4.255007 \n        9        10 \n12.255007 -8.677401 \n\n\n\n\n\n5.2.5 plot()\nplot() will return four plots with regression diagnostics.\n\n(1) Residual plot: This demonstrates if the residuals have non-linear patterns or non-constant variance.\n(2) Normal QQ plot: This demonstrates if the residuals are normally distributed.\n(3) Scale-Location plot: This also demonstrates if the residuals have non-constant variance.\n(4): Residuals vs. leverage plot This demonstrates cases that may be influential.\n\n\nplot(stopping_model)\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot() with lm() is quick but the results are not attractive and customizing anything is a huge pain."
  },
  {
    "objectID": "chapters/05_linear-regression.html#extract-model-data-with-broom",
    "href": "chapters/05_linear-regression.html#extract-model-data-with-broom",
    "title": "5  Data Viz and Regression in R",
    "section": "5.3 3. Extract Model Data with broom",
    "text": "5.3 3. Extract Model Data with broom\nTo leverage the full power of ggplot2, we need tidy data frames with our data of interest. library(broom) will quickly give us access to these data!\n\n5.3.1 library(broom)\nlibrary(broom) contains three helpful functions for tidying the output of estimated models. library(broom) is extensible and has methods for many models (lm(), glm(), kmeans(), LDA()). We will demonstrate applications with lm():\n\naugment() returns one row per observation in the estimation data and includes information like predicted values and residuals.\ntidy() returns one row per coefficient and includes information like point estimates and standard errors.\nglance() returns one row per model and includes information like \\(R^2\\).\n\n\n\naugment()\naugment() returns observation-level diagnostics like residuals and hat values.\n\naugment(stopping_model)\n\n# A tibble: 50 × 8\n    dist speed .fitted .resid   .hat .sigma  .cooksd .std.resid\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1     2     4   -1.85   3.85 0.115    15.5 0.00459       0.266\n 2    10     4   -1.85  11.8  0.115    15.4 0.0435        0.819\n 3     4     7    9.95  -5.95 0.0715   15.5 0.00620      -0.401\n 4    22     7    9.95  12.1  0.0715   15.4 0.0255        0.813\n 5    16     8   13.9    2.12 0.0600   15.5 0.000645      0.142\n 6    10     9   17.8   -7.81 0.0499   15.5 0.00713      -0.521\n 7    18    10   21.7   -3.74 0.0413   15.5 0.00133      -0.249\n 8    26    10   21.7    4.26 0.0413   15.5 0.00172       0.283\n 9    34    10   21.7   12.3  0.0413   15.4 0.0143        0.814\n10    17    11   25.7   -8.68 0.0341   15.5 0.00582      -0.574\n# ℹ 40 more rows\n\n\n\n\nResidual plot\nA residual plot compares fitted values and residuals. It is a useful diagnostic to see if there are non-linear patterns that are not captured by the model and to check for constant error variance.\n\n\n\n\n\n\nExercise 1\n\n\n\nLet’s estimate a model using a subset of the diamonds data set and then create a residual plot.\n\n# sample 300 observations and set ordinal factors to nominal\nset.seed(20200622)\n\ndiamonds &lt;- diamonds %&gt;%\n  slice_sample(n = 300) %&gt;%\n  mutate(across(where(is.factor), .fns = as.character))\n\n\n# estimate a multiple linear regression model\ndiamonds_model1 &lt;- lm(formula = price ~ carat + cut, data = diamonds)\n\nclass(diamonds_model1)\n\n[1] \"lm\"\n\n\nStep 1: Run the above code to estimate a linear regression model on a subset of the diamonds data.\nStep 2: Use augment() to create a data frame with one row per observation in the training data.\nStep 3: Create a scatter plot to compare .fitted and .resid. Add geom_smooth().\n\n\n\n\n\ntidy()\ntidy() returns coefficient-level diagnostics like standard errors and p-values.\n\ntidy(stopping_model)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -17.6      6.76      -2.60 1.23e- 2\n2 speed           3.93     0.416      9.46 1.49e-12\n\n\n\n\nCoefficient plot\nHere’s a simple plot of estimated OLS coefficients and their 95% confidence intervals.\n\ndiamonds_model1_coefs &lt;- tidy(\n  diamonds_model1, \n  conf.int = TRUE,\n  conf.level = 0.95\n) \n\ndiamonds_model1_coefs %&gt;%\n  ggplot(aes(x = estimate, \n             y = term,\n             xmin = conf.low,\n             xmax = conf.high)) +\n  geom_vline(xintercept = 0) +\n  geom_pointrange() +\n  scale_x_continuous(\n    limits = c(-10000, 10000),\n    labels = scales::dollar\n  )\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nLet’s estimate a second regression model on the diamonds data set and then compare the models with a coefficient plot.\nStep 1: Create diamonds_model2 with price as the dependent variable and carat, cut, and x as independent variables.\nStep 2: Use tidy() to create diamonds_model2_coefs. Combine the results using the following code:\n\nmodels_coefs &lt;- bind_rows(\n  `model1` = diamonds_model1_coefs,\n  `model2` = diamonds_model2_coefs,\n  .id = \"model\"\n)\n\nStep 3: Create a coefficient plot with models_coefs. Include color = model.\nStep 4: Add position = position_dodge(width = 0.5) to geom_pointrange().\n\n\n\nMichael Correll and Michael Gleicher have an interesting paper (preprint here) called “Error Bars Considered Harmful: Exploring Alternate Encodings for Mean and Error”. Consider this figure from their paper:\n\n\n\n\n\nHere, (a) suffers from within the bar bias, and (a) and (b) suffers from issues with binary interpretation. It’s tricky to fully adopt (c) or (d), which are visually symmetric and visually continuous, but I never use bars for coefficients and I never use the “capital I” error bars.\nNote: sometimes (a) is referred to as a dynamite plot.\n\n\n\nglance()\nglance() returns model-level diagnostics like \\(R^2\\) and \\(\\hat{\\sigma}\\).\n\nglance(stopping_model)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.651         0.644  15.4      89.6 1.49e-12     1  -207.  419.  425.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nIt isn’t interesting to visualize one model using glance(). However, glance() allows for the comparison of many models.\n\n\nDetailed Example: Creating Many models\nHadley Wickham gave a great talk about estimating many models to the The Edinburgh R User Group. (The relevant sections begins around the 28-minute mark). Here is an example based on his talk:\n\nlibrary(gapminder)\n\nglimpse(gapminder)\n\nRows: 1,704\nColumns: 6\n$ country   &lt;fct&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", …\n$ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, …\n$ year      &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, …\n$ lifeExp   &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8…\n$ pop       &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12…\n$ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, …\n\n\nThe gapminder data set contains information about life expectancy, population, and per-capita GDP over time for every country in the world. It comes from Hans Rosling and the Gapminder Foundation.\nWe can estimate a simple linear regression for every country in the data set with year as the predictor and lifeExp as the outcome variable.\n\n# estimate a linear model for each country\nmany_models &lt;- gapminder %&gt;%\n  group_by(country, continent) %&gt;%\n  # create a nested data frame for each county\n  nest(data = c(year, lifeExp, pop, gdpPercap)) %&gt;%\n  # iterate down each row and estimate a model with the nested data frame\n  mutate(\n    model = map(\n      .x = data, \n      .f = ~glance(lm(formula = lifeExp ~ year, data = .x))\n    )\n  ) %&gt;%\n  ungroup()\n    \n# extract r^2 from each model\nmany_models_results &lt;- many_models %&gt;%\n  mutate(r_squared = map_dbl(model, \"r.squared\"))\n\n# plot\nmany_models_results %&gt;%\n  # reorder the data based on r_squared\n  mutate(country = forcats::fct_reorder(.f = country, .x = r_squared)) %&gt;%\n  ggplot(mapping = aes(r_squared, country, color = continent)) +\n  geom_point(alpha = 0.5)\n\n\n\n\nmap() functions come from library(purrr) and are based on the Map-Reduce framework. This is a functional approach to iteration that replaces for loops. I recommend reading more here.\nCategorical variables are displayed in alphanumeric order by default. fct_reorder() from library(forcats) converts country to a factor and orders it based on the values of r_squared. library(forcats)has several useful functions for ordering categorical axes with library(ggplot2).\nLet’s clean this up a little:\n\nbind_rows(\n  `High R-Squared` = slice_max(many_models_results, r_squared, n = 15),\n  `Low R-Squared` = slice_min(many_models_results, r_squared, n = 15),\n  .id = \"R-Squared\"\n) %&gt;%\n  mutate(country = forcats::fct_reorder(.f = country, .x = r_squared)) %&gt;%\n  ggplot(mapping = aes(r_squared, country, color = continent)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~ `R-Squared`, nrow = 2, scales = \"free_y\")\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\nStep 1: Like above, estimate many models on the diamonds data set. Group by color. You will need to list columns in the nest() functions. Use formula = price ~ carat + cut.\nStep 2: Extract \\(R^2\\) from each model.\nStep 3: Visualize the \\(R^2\\) with a scatter plot with r_squared on the x axis and color on the y axis.\nStep 4: Add scale_x_continuous(limits = c(0, 1))."
  },
  {
    "objectID": "chapters/05_linear-regression.html#create-data-with-librarymodelr",
    "href": "chapters/05_linear-regression.html#create-data-with-librarymodelr",
    "title": "5  Data Viz and Regression in R",
    "section": "5.4 4. Create Data with library(modelr)",
    "text": "5.4 4. Create Data with library(modelr)\nlibrary(modelr) has many useful functions for modeling. It works with more types of models than just linear models from lm().\n\n5.4.1 add_predictions()\nadd_predictions() adds predictions to a data set using an estimated model object.\n\nadd_predictions(data = diamonds, model = diamonds_model1)\n\n# A tibble: 300 × 11\n   carat cut       color clarity depth table price     x     y     z   pred\n   &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1  0.7  Very Good I     VVS1     62.9    58  2513  5.65  5.67  3.56  3070.\n 2  1.52 Ideal     I     SI2      61.7    57  7582  7.38  7.41  4.56 10154.\n 3  0.31 Ideal     H     SI1      61.3    55   421  4.35  4.39  2.68   198.\n 4  0.71 Ideal     D     VS2      59.9    57  3540  5.8   5.83  3.48  3489.\n 5  2.07 Ideal     I     SI2      62      55 16189  8.12  8.22  5.07 14680.\n 6  1    Ideal     E     SI2      61.9    56  4760  6.43  6.4   3.97  5876.\n 7  1.2  Ideal     G     SI1      61.2    56  7920  6.87  6.88  4.21  7521.\n 8  0.52 Good      H     VS2      63.7    54  1385  5.12  5.09  3.25  1409.\n 9  0.65 Very Good G     VS2      63.2    57  2009  5.54  5.47  3.48  2659.\n10  0.49 Premium   F     SI2      62.2    59  1073  5.09  5.04  3.15  1392.\n# ℹ 290 more rows\n\n\n\n\n5.4.2 add_residuals()\nadd_residuals() adds residuals to a data set using an estimated model object.\n\nadd_residuals(data = diamonds, model = diamonds_model1)\n\n# A tibble: 300 × 11\n   carat cut       color clarity depth table price     x     y     z   resid\n   &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1  0.7  Very Good I     VVS1     62.9    58  2513  5.65  5.67  3.56  -557. \n 2  1.52 Ideal     I     SI2      61.7    57  7582  7.38  7.41  4.56 -2572. \n 3  0.31 Ideal     H     SI1      61.3    55   421  4.35  4.39  2.68   223. \n 4  0.71 Ideal     D     VS2      59.9    57  3540  5.8   5.83  3.48    50.6\n 5  2.07 Ideal     I     SI2      62      55 16189  8.12  8.22  5.07  1509. \n 6  1    Ideal     E     SI2      61.9    56  4760  6.43  6.4   3.97 -1116. \n 7  1.2  Ideal     G     SI1      61.2    56  7920  6.87  6.88  4.21   399. \n 8  0.52 Good      H     VS2      63.7    54  1385  5.12  5.09  3.25   -23.9\n 9  0.65 Very Good G     VS2      63.2    57  2009  5.54  5.47  3.48  -650. \n10  0.49 Premium   F     SI2      62.2    59  1073  5.09  5.04  3.15  -319. \n# ℹ 290 more rows\n\n\n\n\n5.4.3 data_grid()\ndata_grid() creates an evenly-spaced grid of points using the range of observed predictors in a data set. This is useful for visualization and is really, really useful for understanding generalized linear models. seq_range() can be used with data_grid() to add a finer grid of values.\n\ndata_grid(data = diamonds, carat, cut) %&gt;%\n  add_predictions(diamonds_model1)\n\n# A tibble: 465 × 3\n   carat cut         pred\n   &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1  0.2  Fair      -2010.\n 2  0.2  Good      -1224.\n 3  0.2  Ideal      -707.\n 4  0.2  Premium    -995.\n 5  0.2  Very Good -1044.\n 6  0.23 Fair      -1763.\n 7  0.23 Good       -977.\n 8  0.23 Ideal      -460.\n 9  0.23 Premium    -748.\n10  0.23 Very Good  -797.\n# ℹ 455 more rows\n\n\n\ncut_levels &lt;- c(\"Fair\", \"Good\",  \"Very Good\", \"Ideal\", \"Premium\")\n\ndata_grid(data = diamonds, carat, cut) %&gt;%\n  add_predictions(diamonds_model1) %&gt;%\n  mutate(cut = factor(cut, levels = cut_levels)) %&gt;%\n  ggplot(aes(x = carat, y = pred, color = cut)) +\n  geom_line(alpha = 0.5) +\n  scale_x_continuous(\n    limits = c(0, 3),\n    expand = c(0, 0)\n  ) +\n  scale_y_continuous(\n    limits = c(-5000, 20000),\n    expand = c(0, 0),\n    labels = scales::dollar\n  ) +\n  labs(title = \"data_grid() is useful for interpreting a regression line\")\n\n\n\n\nCategorical variables are displayed in alphanumeric order by default. Here, we use factor() to give cut a meaningful order: “Fair”, “Good”, “Very Good”, “Ideal”, and “Premium”.\n\n\n\n5.4.4 Complex Models and GLMs\nlibrary(modelr) is useful for visualizing complex models such as polynomial regression or generalized linear models (GLMs) like logistic regression.\n\nset.seed(20201005)\n\n# simulate a predictor\nx1 &lt;- runif(n = 1000, min = 0, max = 10)\n\n# simulate the outcome and create a tibble\nsim_data &lt;- bind_cols(\n  x1 = x1,\n  y = 10 * sin(x1) + 20 + rnorm(n = length(x1), mean = 0, sd = 2)\n)\n\n# plot\nsim_data %&gt;%\n  ggplot(aes(x1, y)) +\n  geom_point(alpha = 0.1)\n\n\n\n\nLet’s fit a 4th-degree polynomial and then add a line for the conditional mean. We could also use augment() in this case because the x1 is dense. If there are gaps in predictors, then data_grid() is necessary.\n\n# fit a model with a 4th-degree polynomial\nlm_4 &lt;- sim_data %&gt;%\n  lm(formula = y ~ poly(x1, degrees = 4, raw = TRUE), data = .)\n\n# create a grid andd predictions\nconditional_mean &lt;- data_grid(sim_data, x1) %&gt;%\n  add_predictions(lm_4)\n\n# plot\nggplot() +\n  geom_point(\n    data = sim_data,\n    mapping = aes(x1, y),\n    alpha = 0.1\n  ) +\n  geom_line(\n    data = conditional_mean,\n    mapping = aes(x1, pred),\n    color = \"red\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\nThis example will demonstrate plotting predicted probabilities for a simple logistic regression model.\nStep 1: Run the following to create a binary outcome variable.\ncars &lt;- cars %&gt;%\n  mutate(crash = as.numeric(dist &gt; 25))\n\nStep 2: Using lm(), estimate a linear regression model (linear probability model) with crash as the outcome variable and speed as the predictor. Call it cars_lm.\nStep 3: Run the following to estimate a logistic regression model:\ncars_glm &lt;- glm(factor(crash) ~ speed, data = cars, family = \"binomial\")\n\nStep 4: Use data_grid() to create a new data frame. Use speed = seq_range(speed, 1000) to make a consistent grid of values for speed.\nStep 5: Create a data frame with conditional probabilities for both models with the following code:\nmodels &lt;- data_grid(cars, speed = seq_range(speed, 1000)) %&gt;%\n  add_predictions(cars_lm, var = \"lm\") %&gt;%\n  add_predictions(cars_glm, type = \"response\", var = \"glm\")\nStep 6: Plot the predicted probabilities for the linear probability model and logistic regression model. I used three layers: geom_point(), geom_line() with y = lm, and geom_line() with y = glm."
  },
  {
    "objectID": "chapters/05_linear-regression.html#summary-1",
    "href": "chapters/05_linear-regression.html#summary-1",
    "title": "5  Data Viz and Regression in R",
    "section": "5.5 Summary",
    "text": "5.5 Summary\nHopefully these notes demonstrate the power of using a tool (R) that has powerful modeling tools and powerful visualization tools. There are several approaches to visualizing regression models.\nWe can model with ggplot2 or use base R to quickly visualize our data. With a little more work, we can use library(broom) and library(modelr) to create tidy data frames and leverage the full power of ggplot2.\n\nCreate models with ggplot2\nUse Base R to Explore lm Objects\nExtract Model Data with broom\nCreate Data with library(modelr)"
  },
  {
    "objectID": "chapters/06_data-munging.html#import-packages-and-functions",
    "href": "chapters/06_data-munging.html#import-packages-and-functions",
    "title": "6  Data Munging for Data Visualization",
    "section": "6.1 Import Packages and Functions",
    "text": "6.1 Import Packages and Functions\n\nfilter() - drop cases based on logical conditions\nmutate() - add new variables are transform existing variables\npivot_wider() - make data wider\npivot_longer() - make data longer\nseparate() - split a character variable into multiple columns based on a delimiter"
  },
  {
    "objectID": "chapters/06_data-munging.html#tidy-data",
    "href": "chapters/06_data-munging.html#tidy-data",
    "title": "6  Data Munging for Data Visualization",
    "section": "6.2 Tidy Data",
    "text": "6.2 Tidy Data\nThis data comes from library(tidyr) and these examples largely come from chapter 12 in R for Data Science by Hadley Wickham and Garrett Grolemund.\n\ntable1, table2, table3, table4a, table4b, and table5 all display the number of TB cases documented by the World Health Organization in Afghanistan, Brazil, and China between 1999 and 2000.\n\nThe following table, table1 is tidy because\n\nEach variable has its own column\nEach observation has its own row\nEach value has its own cell\n\n\ntable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583"
  },
  {
    "objectID": "chapters/06_data-munging.html#untidy-table2",
    "href": "chapters/06_data-munging.html#untidy-table2",
    "title": "6  Data Munging for Data Visualization",
    "section": "6.3 Untidy table2",
    "text": "6.3 Untidy table2\n\nUntidyCleaned\n\n\nWhy aren’t the data tidy?\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\n\n\nEach observation was spread across two rows!\n\ntable2 %&gt;%\n  pivot_wider(names_from = type, values_from = count)\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583"
  },
  {
    "objectID": "chapters/06_data-munging.html#untidy-table3",
    "href": "chapters/06_data-munging.html#untidy-table3",
    "title": "6  Data Munging for Data Visualization",
    "section": "6.4 Untidy table3",
    "text": "6.4 Untidy table3\n\nUntidyCleaned\n\n\nWhy aren’t the data tidy?\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\n\n\nThe rate column had two variables!\n\ntable3 %&gt;%\n  separate(rate, into = c(\"cases\", \"population\")) %&gt;%\n  mutate(\n    cases = as.numeric(cases),\n    population = as.numeric(population)\n  )\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583"
  },
  {
    "objectID": "chapters/06_data-munging.html#untidy-table4",
    "href": "chapters/06_data-munging.html#untidy-table4",
    "title": "6  Data Munging for Data Visualization",
    "section": "6.5 Untidy table4",
    "text": "6.5 Untidy table4\n\nUntidyCleaned\n\n\nWhy aren’t the data tidy?\n\ntable4a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\ntable4b\n\n# A tibble: 3 × 3\n  country         `1999`     `2000`\n  &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\n\n\n\nEach table had variables in the column names and the data were spread across two tables\n\n# fix table4a\ntable4a_fixed &lt;- table4a %&gt;%\n  pivot_longer(\n    cols = c(`1999`, `2000`), \n    names_to = \"year\", \n    values_to = \"cases\"\n  )\n\n# fix table4b\ntable4b_fixed &lt;- table4b %&gt;%\n  pivot_longer(\n    cols = c(`1999`, `2000`), \n    names_to = \"year\", \n    values_to = \"population\"\n  )\n\n# join the two tables into one tidy table\nleft_join(\n  table4a_fixed, \n  table4b_fixed, \n  by = c(\"country\", \"year\")\n)\n\n# A tibble: 6 × 4\n  country     year   cases population\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan 1999     745   19987071\n2 Afghanistan 2000    2666   20595360\n3 Brazil      1999   37737  172006362\n4 Brazil      2000   80488  174504898\n5 China       1999  212258 1272915272\n6 China       2000  213766 1280428583"
  },
  {
    "objectID": "chapters/06_data-munging.html#titanic-data",
    "href": "chapters/06_data-munging.html#titanic-data",
    "title": "6  Data Munging for Data Visualization",
    "section": "6.6 Titanic Data",
    "text": "6.6 Titanic Data\nConsider the Titanic data set from the 3D plots example.\n\ntribble(\n  ~Class, ~Sex, ~n,\n  \"1st class\", \"female passengers\", 144,\n  \"1st class\", \"male passengers\", 179,\n  \"2nd class\", \"female passengers\", 106,\n  \"2nd class\", \"male passengers\", 171, \n  \"3rd class\", \"female passengers\", 216,\n  \"3rd class\", \"male passengers\", 493\n)\n\n\n\n\n\n\n\nExercise 1\n\n\n\nTidy the following data:\n\ntribble(\n  ~Class, ~female_passengers, ~male_passengers,\n  \"1st class\", 144, 179,\n  \"2nd class\", 106, 171,\n  \"3rd class\", 216, 493\n)\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nTidy the following data:\n\ntribble(\n  ~Class, ~`male passengers/female passengers`,\n  \"1st class\", \"179|144\",\n  \"2nd class\", \"171|106\",\n  \"3rd class\", \"493|216\",\n)\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\nSuppose we want to create a column chart with only female passengers. Furthermore, we want n on the x-axis and we want Class on the y-axis with the levels going from 1st to 3rd from top to bottom.\nStep 1: Use filter() to drop male passengers.\nStep 2: Create a column chart with n on the x-axis and Class on the y-axis. Pipe (%&gt;%) the data into ggplot().\nStep 3: After the filter(), add the following:\n  mutate(Class = factor(Class, levels = c(\"3rd class\", \"2nd class\", \"1st class\"))) %&gt;%"
  },
  {
    "objectID": "chapters/06_data-munging.html#additional-tools",
    "href": "chapters/06_data-munging.html#additional-tools",
    "title": "6  Data Munging for Data Visualization",
    "section": "6.7 Additional tools",
    "text": "6.7 Additional tools\n\nlibrary(stringr) has powerful tools for dealing with text strings.\nlibrary(lubridate) has powerful tools for dealing with time and dates."
  },
  {
    "objectID": "chapters/07_time-series.html",
    "href": "chapters/07_time-series.html",
    "title": "7  Time Series and Annotations in R",
    "section": "",
    "text": "8 Introduction\nThis guide is an introduction to visualizing time series data in R and improving data visualization annotations. In this case, time series just means long data with a date variable or date-time variable. R has a native time series object and a “tidy” tsibble object. This training will not address those types of data.\nIt is straightforward to create line plots with a variable of type \"Date\" on the x-axis. Let’s combine two series about unemployment from FRED into one data set.\n# https://fred.stlouisfed.org/series/UNRATE\n# accessed on 2021-11-19\nu3_rate &lt;- read_csv(here(\"data\", \"UNRATE.csv\"))\n\n# https://fred.stlouisfed.org/series/U6RATE\n# accessed on 2021-11-19\nu6_rate &lt;- read_csv(here(\"data\", \"U6RATE.csv\"))\n\n# combine the U3 and U6 data\nrates &lt;- left_join(u3_rate, u6_rate, by = \"DATE\")\nPlot!\nrates %&gt;%\n  ggplot(mapping = aes(x = DATE, y = UNRATE)) +\n  geom_line()\nLet’s clean this up some. We divide UNRATE by 100 because ggplot2 works better with proportions.\nrates %&gt;%\n  mutate(UNRATE = UNRATE / 100) %&gt;%\n  ggplot(mapping = aes(x = DATE, y = UNRATE)) +\n  geom_line() +\n  scale_y_continuous(\n    limits = c(0, NA),\n    labels = scales::percent_format(accuracy = 1)\n  ) +\n  labs(\n    title = \"The Unemployment Rate Spiked After the Emergence of COVID-19\",\n    subtitle = \"\",\n    x = \"Date\",\n    y = \"Unemployment Rate\",\n    caption = \"Source: Bureau of Labor Statistics data accessed through FRED\"\n  )\nExercise 2 is tedious. What if we want to show the U3 and U6 unemployment rates on the same plot without repeating geom_line()? library(ggplot2) expects “long” data for this and we will need to use pivoting.\npivot_longer() performs this operation. First, state the columns you want (or don’t want) to pivot. Second, name the new column that will contain the old column headers with names_to =. Third, name the new column where the cell values will go with values_to =.\nThe following code pivots every column except DATE, sends the old column names to a new column called series, and sends the old cell values to a column called rate.\nWe map variables to aesthetic mappings. Now series is a variable and we can map it to the color aesthetic.\nrates %&gt;%\n  pivot_longer(\n    cols = -DATE, \n    names_to = \"series\", \n    values_to = \"rate\"\n  ) %&gt;%\n  ggplot(mapping = aes(x = DATE, y = rate, color = series)) +\n  geom_line()\nThe inverse of pivot_longer() is pivot_wider().\nrates\n\n# A tibble: 886 × 3\n   DATE       UNRATE U6RATE\n   &lt;date&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n 1 1948-01-01    3.4     NA\n 2 1948-02-01    3.8     NA\n 3 1948-03-01    4       NA\n 4 1948-04-01    3.9     NA\n 5 1948-05-01    3.5     NA\n 6 1948-06-01    3.6     NA\n 7 1948-07-01    3.6     NA\n 8 1948-08-01    3.9     NA\n 9 1948-09-01    3.8     NA\n10 1948-10-01    3.7     NA\n# ℹ 876 more rows\n\nrates %&gt;%\n  pivot_longer(\n    cols = -DATE, \n    names_to = \"series\", \n    values_to = \"rate\"\n  ) %&gt;%\n  pivot_wider(\n    names_from = series, \n    values_from = rate\n  )\n\n# A tibble: 886 × 3\n   DATE       UNRATE U6RATE\n   &lt;date&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n 1 1948-01-01    3.4     NA\n 2 1948-02-01    3.8     NA\n 3 1948-03-01    4       NA\n 4 1948-04-01    3.9     NA\n 5 1948-05-01    3.5     NA\n 6 1948-06-01    3.6     NA\n 7 1948-07-01    3.6     NA\n 8 1948-08-01    3.9     NA\n 9 1948-09-01    3.8     NA\n10 1948-10-01    3.7     NA\n# ℹ 876 more rows\nNote: pivot_longer() and pivot_wider() replace older functions like gather()/spread() and melt()/cast().\nIt is useful to shade regions on a time series plot to show events or periods like recessions.\nFRED contains dates for recession bars based on the NBER’s business cycle turning points. Here, I copied the dates and turned them into a data frame.\n# https://fredhelp.stlouisfed.org/fred/data/understanding-the-data/recession-bars/\n# downloaded from FRED on 2021-11-19\n\nrecessions &lt;- read_csv(\n  \"Peak, Trough\n   1857-06-01, 1858-12-01\n   1860-10-01, 1861-06-01\n   1865-04-01, 1867-12-01\n   1869-06-01, 1870-12-01\n   1873-10-01, 1879-03-01\n   1882-03-01, 1885-05-01\n   1887-03-01, 1888-04-01\n   1890-07-01, 1891-05-01\n   1893-01-01, 1894-06-01\n   1895-12-01, 1897-06-01\n   1899-06-01, 1900-12-01\n   1902-09-01, 1904-08-01\n   1907-05-01, 1908-06-01\n   1910-01-01, 1912-01-01\n   1913-01-01, 1914-12-01\n   1918-08-01, 1919-03-01\n   1920-01-01, 1921-07-01\n   1923-05-01, 1924-07-01\n   1926-10-01, 1927-11-01\n   1929-08-01, 1933-03-01\n   1937-05-01, 1938-06-01\n   1945-02-01, 1945-10-01\n   1948-11-01, 1949-10-01\n   1953-07-01, 1954-05-01\n   1957-08-01, 1958-04-01\n   1960-04-01, 1961-02-01\n   1969-12-01, 1970-11-01\n   1973-11-01, 1975-03-01\n   1980-01-01, 1980-07-01\n   1981-07-01, 1982-11-01\n   1990-07-01, 1991-03-01\n   2001-03-01, 2001-11-01\n   2007-12-01, 2009-06-01\n   2020-02-01, 2020-04-01\"\n)\nWe can use geom_rect() to add the shaded regions to a plot. Unlike most other geometric objects we’ve used, geom_rect() has four required arguments: xmin, xmax, ymin, and ymax.\nrecessions %&gt;%\n  ggplot(aes(xmin = Peak, xmax = Trough, ymin = 0, ymax = 1)) +\n  geom_rect(alpha = 0.3)\nWe used geom_text() to label bars for Jon Schwabish’s “Guideline 3: Integrate the Graphics and Text”. We also used geom_label() to label lines for Jon Schwabish’s “Guideline 5: Start with Gray”.\nGeometric objects require at least one variable in a data frame. Sometimes we just want to add a label or a single shaded region without needing an entire data set. annotate() can add individual text annotations or shaded regions.\nThe Ames Housing data set contains house prices and features in Ames, Iowa. It is common data set for predictive modeling. Three homes are particularly difficult to predict because they are unfinished.\nlibrary(AmesHousing)\n\names &lt;- make_ames()\n\names %&gt;%\n  mutate(\n    square_footage = Total_Bsmt_SF - Bsmt_Unf_SF + First_Flr_SF + Second_Flr_SF\n  ) %&gt;%\n  ggplot(aes(square_footage, Sale_Price)) +\n  geom_point(alpha = 0.2, color = \"#1696d2\") +\n  scale_x_continuous(\n    expand = expansion(mult = c(0, 0.002)),\n    limits = c(-10, 12000),\n    labels = scales::comma\n  ) + \n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.002)),\n    limits = c(0, 800000),\n    labels = scales::dollar\n  ) +  \n  annotate(\"rect\", xmin = 6800, xmax = 11500, ymin = 145000, ymax = 210000, alpha = 0.1) +\n  annotate(\"text\", x = 8750, y = 230000, label = \"Unfinished homes\") +\n  labs(\n    x = \"Square footage\", \n    y = \"Sale price\"\n  ) +\n  theme(plot.margin = margin(t = 6, r = 14, b = 6, l = 6))\nBen Casselman from the New York Times does great data visualization Twitter threads every “jobs day”. His data visualizations are created in R and his process is fairly automated at this point. Let’s consider this example, which includes a simple linear trend line.\n# https://fred.stlouisfed.org/series/PAYEMS\n# accessed on 2021-11-19\n\npayrolls &lt;- read_csv(here(\"data\", \"PAYEMS.csv\")) %&gt;%\n  mutate(PAYEMS = PAYEMS / 1000)\n\n# filter to pre-pandemic data for model estimation\npayrolls_pre_pandemic &lt;- payrolls %&gt;%\n  filter(DATE &lt; \"2020-03-31\")\n\n# fit a simple linear regression model with DATE as the predictor\npre_pandemic_trend &lt;- lm(PAYEMS ~ DATE, data = payrolls_pre_pandemic)\n\n# extract coefficients\npre_pandemic_coefs &lt;- coef(pre_pandemic_trend)\n\n# plot\nggplot() +\n  geom_line(\n    data = payrolls,\n    aes(DATE, PAYEMS)\n  ) +\n  geom_abline(\n    intercept = pre_pandemic_coefs[\"(Intercept)\"], \n    slope = pre_pandemic_coefs[\"DATE\"],\n    linetype = \"dashed\"\n  ) +\n  scale_y_continuous(limits = c(NA, 160))\nInstead of using geom_abline(), we could have used broom() or predict() and added a second layer with geom_line().\nSometimes we have forecasts and we want to show many possible outcomes or a range of possible outcomes.\nLet’s start with the six months of data from VTI, Vanguard’s total U.S. stock market ETF.\nlibrary(tidyquant)\n\nvti &lt;- tq_get(\n  from = \"2021-01-01\",\n  to = \"2021-06-30\",\n  x  = \"VTI\"\n)\nLet’s add a random walk for 100 days after the known first six months. This function takes a date and a closing price and then adds 100 days of a random walk with mean mean = 1.001 and sd = 0.01. This means the forecast has a modest upward slope and a fair amount of variance.\n#' Add a random walk to a stock\n#'\n#' @param iteration An id for the random walk\n#' @param last_date A date for the last date of observed data\n#' @param last_close A numeric for the last closing price\n#'\n#' @return A tidy data frame with iteration id, data, and price\n#' \nforecast &lt;- function(iteration, last_date, last_close) {\n  \n  start_date &lt;- date(last_date) + days(1)\n  end_date &lt;- date(last_date) + days(100)\n  \n  tibble(\n    date = c(date(last_date), seq(start_date, end_date, by = \"day\")),\n    forecast_close = cumprod(c(last_close, rnorm(100, mean = 1.001, sd = 0.01))),\n    iteration = iteration\n  )\n  \n}\nOne random walk is only so interesting. Let’s iterate the random walk 1,001 times with map_dfr(). map() functions come from library(purrr) and are based on the Map-Reduce framework. This is a functional approach to iteration that replaces for loops. I recommend reading more here.\nforecasts &lt;- map_dfr(\n  .x = 1:1001, \n  .f = ~forecast(\n    iteration = .x, \n    last_date = \"2021-06-30\", \n    last_close = pull(slice_max(vti, date), close)\n  )\n)\nNow we can visualize. First we add a layer with the historical data. Second, we add the random walks. Here group = iteration links the points together into lines from each random walk.\nggplot() +\n  geom_line(\n    data = vti,\n    mapping = aes(date, close)\n  ) +\n  geom_line(\n    data = forecasts,\n    mapping = aes(date, forecast_close, group = iteration),\n    alpha = 0.05\n  )\nSuppose we want to highlight the median forecast in blue and add the daily average in red.\n# calculate the daily mean\ndaily_mean &lt;- forecasts %&gt;%\n  group_by(date) %&gt;%\n  summarize(forecast_close = mean(forecast_close))\n\n# find the median forecast\n# note: this is trickier if we have an even  number of forecasts\nmedian_forecast &lt;- forecasts %&gt;%\n  slice_max(date) %&gt;%\n  filter(median(forecast_close) == forecast_close) %&gt;%\n  pull(iteration)\n\n# plot (with four layers)!\nggplot() +\n  geom_line(\n    data = vti,\n    mapping = aes(date, close)\n  ) +\n  geom_line(\n    data = forecasts,\n    mapping = aes(date, forecast_close, group = iteration),\n    alpha = 0.05\n  ) +\n  geom_line(\n    data = daily_mean,\n    mapping = aes(date, forecast_close),\n    color = \"red\"\n  ) +\n  geom_line(\n    data = filter(forecasts, iteration == median_forecast),\n    mapping = aes(date, forecast_close),\n    color = \"blue\"\n  ) +\n  labs(subtitle = \"Daily average in red, median forcaset in blue\")\nWhat if we want to show intervals instead of lines. First, we can group by date and calculate 80%, 95%, and 99% intervals for each day.\nintervals &lt;- forecasts %&gt;%\n  group_by(date) %&gt;%\n  summarize(\n    q = c(0.005, 0.025, 0.1, 0.9, 0.975, 0.995),\n    forecast_close = quantile(\n      forecast_close, \n      probs = c(0.005, 0.025, 0.1, 0.9, 0.975, 0.995)\n    )\n  ) %&gt;%\n  ungroup()\nWe need to do some tricky data munging. First, we need to label each quantile as the minimum or maximum of the interval. Second we need to associate each maximum percentile with its maximum. We call this interval. Finally, we pivot the data to be wider so the minimum and maximum are separate variables.\nintervals &lt;- intervals %&gt;%\n  mutate(bound = if_else(q &lt; 0.5, \"minimum\", \"max\")) %&gt;%\n  mutate(\n    interval = case_when(\n      q %in% c(0.005, 0.995) ~ \"99%\",\n      q %in% c(0.025, 0.975) ~ \"95%\",\n      q %in% c(0.1, 0.9) ~ \"80%\"\n    )\n  ) %&gt;%\n  select(-q) %&gt;%\n  pivot_wider(names_from = bound, values_from = forecast_close)\n\nggplot() +\n  geom_line(\n    data = vti,\n    mapping = aes(date, close)\n  ) +\n  geom_ribbon(\n    data = intervals,\n    aes(date, ymin = minimum, max = max, color = interval, fill = interval),\n    alpha = 0.2\n  )"
  },
  {
    "objectID": "chapters/07_time-series.html#dates",
    "href": "chapters/07_time-series.html#dates",
    "title": "7  Time Series and Annotations in R",
    "section": "9.1 Dates",
    "text": "9.1 Dates\nThere are many ways to store dates.\n\nMarch 14, 1992\n03/14/1992\n14/03/1992\n14th of March ’92\n\nOne way of storing dates is the best. The ISO 8601 date format is an international standard with appealing properties like fixed lengths and self ordering. The format is YYYY-MM-DD.\nlibrary(lubridate) has useful functions that will take dates of any format and convert them to the ISO 8601 standard.\n\nlibrary(lubridate)\n\nmdy(\"March 14, 1992\")\n\n[1] \"1992-03-14\"\n\nmdy(\"03/14/1992\")\n\n[1] \"1992-03-14\"\n\ndmy(\"14/03/1992\")\n\n[1] \"1992-03-14\"\n\ndmy(\"14th of March '92\")\n\n[1] \"1992-03-14\"\n\n\nThese functions return variables of class \"Date\".\n\nclass(mdy(\"March 14, 1992\"))\n\n[1] \"Date\""
  },
  {
    "objectID": "chapters/07_time-series.html#date-times",
    "href": "chapters/07_time-series.html#date-times",
    "title": "7  Time Series and Annotations in R",
    "section": "9.2 Date Times",
    "text": "9.2 Date Times\nlibrary(lubridate) also contains functions for parsing date times into ISO 8601 standard. Times are slightly trickier because of time zones.\n\nmdy_hms(\"12/02/2021 1:00:00\")\n\n[1] \"2021-12-02 01:00:00 UTC\"\n\nmdy_hms(\"12/02/2021 1:00:00\", tz = \"EST\")\n\n[1] \"2021-12-02 01:00:00 EST\"\n\nmdy_hms(\"12/02/2021 1:00:00\", tz = \"America/Chicago\")\n\n[1] \"2021-12-02 01:00:00 CST\"\n\n\nBy default, library(lubridate) will put the date times in Coordinated Universal Time (UTC), which is the successor to Greenwich Mean Time (GMT). I recommend carefully reading the data dictionary if time zones are important for your analysis or if your data cross time zones. This is especially important during time changes (e.g. “spring forward” and “fall back”).\nFortunately, if you encode your dates or date-times correctly, then library(lubridate) will automatically account for time changes, time zones, leap years, leap seconds, and all of the quirks of dates and times.\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\ndates &lt;- tribble(\n  ~date,\n  \"12/01/1987\",\n  \"12/02/1987\",\n  \"12/03/1987\"\n)\n\nStep 1: Create the dates data from above with tribble().\nStep 2: Use mutate() to convert the date column to the ISO 8601 standard."
  },
  {
    "objectID": "chapters/08_where-to-go-from-here.html#publications",
    "href": "chapters/08_where-to-go-from-here.html#publications",
    "title": "8  Where to Go From Here!",
    "section": "8.1 Publications",
    "text": "8.1 Publications\n\nR for Data Science by Hadley Wickham and Garrett Grolemund\nggplot2: elegant graphics for data analysis by Hadley Wickham, Danielle Navarro, and Thomas Lin Pedersen\nFundamentals of Data Visualization by Claus O. Wilke\nBetter Data Visualizations: A Guide for Scholars, Researchers, and Wonks by Jon Schwabish\nR Markdown: The Definitive Guide by Yihui Xie, J. J. Allaire, Garrett Grolemund\nfivethirtyeight best and weirdest charts"
  },
  {
    "objectID": "chapters/08_where-to-go-from-here.html#people-and-places",
    "href": "chapters/08_where-to-go-from-here.html#people-and-places",
    "title": "8  Where to Go From Here!",
    "section": "8.2 People and Places",
    "text": "8.2 People and Places\n\nYihui Xie\nMona Chalabi\nData@Urban\nRStudio education"
  },
  {
    "objectID": "chapters/08_where-to-go-from-here.html#tidy-tuesday",
    "href": "chapters/08_where-to-go-from-here.html#tidy-tuesday",
    "title": "8  Where to Go From Here!",
    "section": "8.3 Tidy Tuesday",
    "text": "8.3 Tidy Tuesday\nGitHub\nCheck out #TidyTuesday on Twitter to follow the submissions\n\nA weekly data project aimed at the R ecosystem. As this project was borne out of the R4DS Online Learning Community and the R for Data Science textbook, an emphasis was placed on understanding how to summarize and arrange data to make meaningful charts with ggplot2, tidyr, dplyr, and other tools in the tidyverse ecosystem. However, any code-based methodology is welcome - just please remember to share the code used to generate the results.\n\nDavid Robinson, a great data analyst and R package developer, often records live videos where he talks his way through a #TidyTuesday event."
  },
  {
    "objectID": "chapters/08_where-to-go-from-here.html#getting-help",
    "href": "chapters/08_where-to-go-from-here.html#getting-help",
    "title": "8  Where to Go From Here!",
    "section": "8.4 Getting help",
    "text": "8.4 Getting help\n\n8.4.1 Googling\nWhen Googling for R or data science help, set the search range to the last year or less to avoid out-of-date solutions and to focus on up-to-date practices. The search window can be set by clicking Tools after a Google search.\n\n\n8.4.2 Stack Overflow\nStack Overflow contains numerous solutions. If a problem is particularly perplexing, it is simple to submit questions. Exercise caution when submitting questions because the Stack Overflow community has strict norms about questions and loose norms about respecting novices.\n\n\n8.4.3 RStudio community\nRStudio Community is a new forum for R Users. It has a smaller back catalog than Stack Overflow but users are friendlier than on Stack Overflow.\n\n\n8.4.4 CRAN task views\nCRAN task views contains thorough introductions to packages and techniques organized by subject matter. The Econometrics, Reproducible Research, and and Social Sciences task views are good starting places.\n\n\n8.4.5 Twitter LinkedIn\nPlease connect with me on LinkedIn!"
  },
  {
    "objectID": "chapters/reproducible-research-bootcamp_software-installation.html",
    "href": "chapters/reproducible-research-bootcamp_software-installation.html",
    "title": "Appendix A — Software Requirements",
    "section": "",
    "text": "B Installing R and RStudio\nR is an open source statistical programming language. RStudio is an Integrated Development Environment (IDE) for developing R code. RStudio is developed by the for-profit company Posit. Don’t worry, you don’t need to pay any money for this software."
  },
  {
    "objectID": "chapters/reproducible-research-bootcamp_software-installation.html#windows",
    "href": "chapters/reproducible-research-bootcamp_software-installation.html#windows",
    "title": "Appendix A — Software Requirements",
    "section": "B.1 Windows",
    "text": "B.1 Windows\n\nB.1.1 R\n\nNavigate to the CRAN website.\nClick “Download R for Windows”\nClick “base”\nClick “Download R 4.4.1 for Windows”\nFollow the installation instructions. Accept all defaults and install R.\n\n\n\nB.1.2 RStudio\n\nNavigate to the RStudio website.\nDownload the RStudio Desktop installer for Windows. It should be something similar to “RStudio-2024.04.2-764.exe”.\nFollow the installation instructions. Accept all defaults and install RStudio.\nOpen RStudio. If successful, then R and RStudio are installed."
  },
  {
    "objectID": "chapters/reproducible-research-bootcamp_software-installation.html#mac",
    "href": "chapters/reproducible-research-bootcamp_software-installation.html#mac",
    "title": "Appendix A — Software Requirements",
    "section": "B.2 Mac",
    "text": "B.2 Mac\n\nB.2.1 R\n\nNavigate to the CRAN website.\nClick “Download R for (Mac) OS X”\nSelect the .pkg link under “Latest Release” that corresponds with your chip type (Apple silicon or Intel).\nFollow the installation instructions. Accept all defaults and install R.\n\n\n\nB.2.2 RStudio\n\nNavigate to the RStudio website.\nDownload the RStudio Desktop installer. It should be something similar to “RStudio-2024.04.2-764.dmg”.\nFollow the installation instructions. Accept all defaults and install RStudio.\nOpen RStudio. If successful, then R and RStudio are installed."
  }
]